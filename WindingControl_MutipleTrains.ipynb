{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os, os.path\n",
    "import keras\n",
    "import time\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomUniform, RandomNormal\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resolution of images\n",
    "\n",
    "h = 75\n",
    "w = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of positiv Sample: 8590\n",
      "Size of negativ Sample: 13846\n"
     ]
    }
   ],
   "source": [
    "#Creates list of positiv files\n",
    "positiv_list = glob.glob('/fhgfs/groups/e5/lhcb/detector/scifi/windingcontrol/Images_Sep17/pos/IMG_*.png')\n",
    "#Finds the number of positiv files\n",
    "positiv_len = len(positiv_list)\n",
    "print(\"Size of positiv Sample: {}\".format(positiv_len))\n",
    "\n",
    "#Same but for negativ files\n",
    "negativ_list = glob.glob('/fhgfs/groups/e5/lhcb/detector/scifi/windingcontrol/Images_Sep17/neg/IMG_*.png')\n",
    "negativ_len = len(negativ_list)\n",
    "print(\"Size of negativ Sample: {}\".format(negativ_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8590/8590 [00:17<00:00, 487.71it/s]\n",
      "100%|██████████| 13846/13846 [00:27<00:00, 495.01it/s]\n"
     ]
    }
   ],
   "source": [
    "X_posind = []\n",
    "X_negind = []\n",
    "X_posinu = []\n",
    "X_neginu = []\n",
    "X_posinl = []\n",
    "X_neginl = []\n",
    "X_posinr = []\n",
    "X_neginr = []\n",
    "\n",
    "X_pos = []\n",
    "X_neg = []\n",
    "\n",
    "for fname in tqdm(positiv_list):\n",
    "    img = scipy.misc.imresize(np.array(Image.open(fname)),(h,w))\n",
    "    \n",
    "    #X_posind.append(scipy.ndimage.interpolation.shift(img, (20,0), output=None, order=1, mode='nearest', cval=0.0, prefilter=True))\n",
    "    #X_posinu.append(scipy.ndimage.interpolation.shift(img, (-20,0), output=None, order=1, mode='nearest', cval=0.0, prefilter=True))\n",
    "    \n",
    "    #X_posinr.append(scipy.ndimage.interpolation.shift(img, (0,50), output=None, order=1, mode='reflect', cval=0.0, prefilter=True))\n",
    "    #X_posinl.append(scipy.ndimage.interpolation.shift(img, (0,-50), output=None, order=1, mode='reflect', cval=0.0, prefilter=True))\n",
    "    \n",
    "    X_pos.append(img) \n",
    "    \n",
    "# Random picking of neg images with the given ratio to the pos images\n",
    "ratio_pos_neg = 1\n",
    "\n",
    "for fname in tqdm(negativ_list):\n",
    "#for fname in tqdm(np.random.choice(negativ_list, replace=False, size=int(len(X_pos) / ratio_pos_neg))):\n",
    "    img = scipy.misc.imresize(np.array(Image.open(fname)),(h,w))\n",
    "    \n",
    "    #X_negind.append(scipy.ndimage.interpolation.shift(img, (20,0), output=None, order=1, mode='nearest', cval=0.0, prefilter=True))\n",
    "    #X_neginu.append(scipy.ndimage.interpolation.shift(img, (-20,0), output=None, order=1, mode='nearest', cval=0.0, prefilter=True))\n",
    "    \n",
    "    #X_neginr.append(scipy.ndimage.interpolation.shift(img, (0,50), output=None, order=1, mode='reflect', cval=0.0, prefilter=True))\n",
    "    #X_neginl.append(scipy.ndimage.interpolation.shift(img, (0,-50), output=None, order=1, mode='reflect', cval=0.0, prefilter=True))\n",
    "    \n",
    "    X_neg.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_posind = np.array(X_posind)\n",
    "X_negind = np.array(X_negind)\n",
    "X_posinu = np.array(X_posinu)\n",
    "X_neginu = np.array(X_neginu)\n",
    "X_posinl = np.array(X_posinl)\n",
    "X_neginl = np.array(X_neginl)\n",
    "X_posinr = np.array(X_posinr)\n",
    "X_neginr = np.array(X_neginr)\n",
    "X_pos = np.array(X_pos)\n",
    "X_neg = np.array(X_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = np.concatenate((X_pos, X_neg), axis=0)\n",
    "Y_all = np.concatenate( ( np.ones(len(X_pos)), np.zeros(len(X_neg)) ) , axis=0)\n",
    "#X_all_trans = np.concatenate((X_pos, X_posind, X_posinu, X_posinl, X_posinr, X_neg, X_negind, X_neginu, X_neginl, X_neginr), axis=0)\n",
    "#Y_all_trans = np_utils.to_categorical(np.concatenate((np.ones((5*positiv_len, 1)), np.zeros((5*negativ_len,1))), axis=0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22436, 75, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22436,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffling the order of the input - required to avoid batches selecting only positive/negative images\n",
    "randomize = np.arange(len(X_all))\n",
    "np.random.shuffle(randomize)\n",
    "X_all = X_all[randomize]\n",
    "Y_all = Y_all[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check labeling of Data\n",
    "if False:\n",
    "    for i, img in enumerate(X_all[:200]):\n",
    "        plt.imshow(np.squeeze(img), cmap='gray')\n",
    "\n",
    "        name = \"Good img\" if Y_all[i] == 1 else \"Bad img\"\n",
    "        plt.title('{}.'.format(name))\n",
    "        plt.savefig('../Plots/TestImg/{}.png'.format(i))\n",
    "        if i%10 == 0:\n",
    "            print(\"10 written\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hold 5% back as training data\n",
    "train_split = int(0.05 * len(X_all))\n",
    "\n",
    "# Separate in Training and Testing Sample\n",
    "X_train = X_all[train_split:]\n",
    "Y_train = Y_all[train_split:]\n",
    "\n",
    "X_test = X_all[:train_split]\n",
    "Y_test = Y_all[:train_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check labeling of Data\n",
    "if False:\n",
    "    for i, img in enumerate(X_test[:100]):\n",
    "        plt.imshow(np.squeeze(img), cmap='gray')\n",
    "\n",
    "        name = \"Good img\" if Y_test[i] == 1 else \"Bad img\"\n",
    "        plt.title('{}.'.format(name))\n",
    "        plt.savefig('../Plots/TestImg/{}.png'.format(i))\n",
    "        if i%10 == 0:\n",
    "            print(\"10 written\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21315, 75, 100, 1)\n",
      "(21315,)\n",
      "(1121, 75, 100, 1)\n",
      "(1121,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], h, w, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], h, w, 1)\n",
    "\n",
    "#Shape check (NumberofImages, Height, Width, Depth)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data need to be normalised\n",
    "* Mean and StdDev for each pixel over whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21315, 75, 100, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = np.mean(X_train, axis=0)\n",
    "stds = np.std(X_train, axis=0)\n",
    "\n",
    "#means_30_40 = np.mean(X_train_30_40, axis=0)\n",
    "#stds_30_40 = np.std(X_train_30_40, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 100, 1)\n",
      "(75, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(means.shape)\n",
    "print(stds.shape)\n",
    "#print(means_30_40.shape)\n",
    "#print(stds_30_40.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_meanstd = '../TrainedModels/' + str(datetime.now().strftime('%Y-%m-%d')) + '/MeansStdDev/'\n",
    "\n",
    "if not os.path.exists(path_meanstd):\n",
    "    os.makedirs(path_meanstd)\n",
    "    print('Created path: {}'.format(path_meanstd))\n",
    "                                            \n",
    "#np.savetxt('../Data/Means_30_40.txt', means_30_40)\n",
    "#np.savetxt('../Data/StdDev_30_40.txt', stds_30_40)\n",
    "np.savetxt(path_meanstd +'Means_' +  str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) +  '.txt', means)\n",
    "np.savetxt(path_meanstd +'StdDev_' +  str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) +  '.txt', stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAD8CAYAAADkM2ZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8559442e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(means), cmap='gray')\n",
    "plt.savefig(path_meanstd + 'MeanImg' +  str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAD8CAYAAADkM2ZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe855944048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(stds), cmap='gray')\n",
    "plt.savefig(path_meanstd + 'StdImg' +  str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_norm = np.array([(img-means)/stds for img in X_train])\n",
    "X_test_norm = np.array([(img-means)/stds for img in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the training begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Class to get loss and accuracy during training of NN\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accuracy = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracy.append(logs.get('acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape= (h,w,1)\n",
    "#input_shape_30_40= (x_new,y_new,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* initializer need to have mean = 0 and std 1/input_shape\n",
    "* for successive layers output shape of previous layer will declare this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with 3 Conv Layer and 2 Fully Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_3Conv_2Fully = ['167_165_163_200_200', '165_165_163_200_200', '165_163_163_200_200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_sizes_3Conv_2Fully = [(7,5,3), (5,5,3), (5,3,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 167_165_163_200_200\tKernelsize: (7, 5, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 69, 94, 16)        800       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 34, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 43, 16)        6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 21, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 19, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 9, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 864)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               173000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 222,937\n",
      "Trainable params: 222,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Created path: ../Data/2017-09-16/167_165_163_200_200/\n",
      "Train on 20249 samples, validate on 1066 samples\n",
      "Epoch 1/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.5782 - acc: 0.7086 - val_loss: 0.5120 - val_acc: 0.7627\n",
      "Epoch 2/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.4514 - acc: 0.7906 - val_loss: 0.4066 - val_acc: 0.8180\n",
      "Epoch 3/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.3132 - acc: 0.8604 - val_loss: 0.2359 - val_acc: 0.8893\n",
      "Epoch 4/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.2069 - acc: 0.9194 - val_loss: 0.1541 - val_acc: 0.9353\n",
      "Epoch 5/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.1427 - acc: 0.9470 - val_loss: 0.0895 - val_acc: 0.9690\n",
      "Epoch 6/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0897 - acc: 0.9672 - val_loss: 0.0595 - val_acc: 0.9803\n",
      "Epoch 7/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0562 - acc: 0.9804 - val_loss: 0.0463 - val_acc: 0.9812\n",
      "Epoch 8/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0450 - acc: 0.9833 - val_loss: 0.0443 - val_acc: 0.9841\n",
      "Epoch 9/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0386 - acc: 0.9852 - val_loss: 0.0308 - val_acc: 0.9916\n",
      "Epoch 10/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0264 - acc: 0.9904 - val_loss: 0.0342 - val_acc: 0.9916\n",
      "Epoch 11/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0228 - acc: 0.9921 - val_loss: 0.0356 - val_acc: 0.9925\n",
      "Epoch 12/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0216 - acc: 0.9925 - val_loss: 0.0336 - val_acc: 0.9925\n",
      "Epoch 13/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0182 - acc: 0.9936 - val_loss: 0.1476 - val_acc: 0.9503\n",
      "Epoch 14/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0135 - acc: 0.9952 - val_loss: 0.0205 - val_acc: 0.9972\n",
      "Epoch 15/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0114 - acc: 0.9960 - val_loss: 0.0204 - val_acc: 0.9962\n",
      "Epoch 16/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0126 - acc: 0.9953 - val_loss: 0.0222 - val_acc: 0.9925\n",
      "Epoch 17/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0390 - val_acc: 0.9925\n",
      "Epoch 18/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0117 - acc: 0.9964 - val_loss: 0.0177 - val_acc: 0.9962\n",
      "Epoch 19/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0124 - acc: 0.9961 - val_loss: 0.0143 - val_acc: 0.9981\n",
      "Epoch 20/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0062 - acc: 0.9978 - val_loss: 0.0228 - val_acc: 0.9953\n",
      "Epoch 21/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0084 - acc: 0.9971 - val_loss: 0.0453 - val_acc: 0.9878\n",
      "Epoch 22/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0089 - acc: 0.9967 - val_loss: 0.0176 - val_acc: 0.9962\n",
      "Epoch 23/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0135 - acc: 0.9955 - val_loss: 0.0203 - val_acc: 0.9944\n",
      "Epoch 24/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0197 - val_acc: 0.9962\n",
      "Epoch 25/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0253 - val_acc: 0.9953\n",
      "Epoch 26/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0150 - val_acc: 0.9981\n",
      "Epoch 27/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0200 - val_acc: 0.9934\n",
      "Epoch 28/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0188 - val_acc: 0.9972\n",
      "Epoch 29/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0079 - val_acc: 0.9991\n",
      "Epoch 30/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0095 - val_acc: 0.9981\n",
      "Epoch 31/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0093 - acc: 0.9974 - val_loss: 0.0294 - val_acc: 0.9944\n",
      "Epoch 32/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0171 - val_acc: 0.9972\n",
      "Epoch 33/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0084 - acc: 0.9975 - val_loss: 0.0293 - val_acc: 0.9944\n",
      "Epoch 34/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0218 - val_acc: 0.9981\n",
      "Epoch 35/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0099 - val_acc: 0.9981\n",
      "Epoch 36/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0311 - val_acc: 0.9944\n",
      "Epoch 37/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0206 - val_acc: 0.9962\n",
      "Epoch 38/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0224 - val_acc: 0.9934\n",
      "Epoch 39/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0067 - val_acc: 0.9991\n",
      "Epoch 40/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0194 - val_acc: 0.9972\n",
      "Epoch 41/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0059 - acc: 0.9986 - val_loss: 0.0133 - val_acc: 0.9991\n",
      "Epoch 42/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0164 - val_acc: 0.9981\n",
      "Epoch 43/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0101 - val_acc: 0.9991\n",
      "Epoch 44/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0112 - val_acc: 0.9991\n",
      "Epoch 45/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0141 - val_acc: 0.9991\n",
      "Epoch 46/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0193 - val_acc: 0.9981\n",
      "Epoch 47/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0035 - acc: 0.9992 - val_loss: 0.0108 - val_acc: 0.9991\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 66s - loss: 0.0122 - acc: 0.9964 - val_loss: 0.0171 - val_acc: 0.9981\n",
      "Epoch 49/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0460 - val_acc: 0.9916\n",
      "Epoch 50/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0165 - val_acc: 0.9972\n",
      "Epoch 51/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0172 - val_acc: 0.9981\n",
      "Epoch 52/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0051 - acc: 0.9989 - val_loss: 0.0642 - val_acc: 0.9897\n",
      "Epoch 53/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0105 - acc: 0.9969 - val_loss: 0.0373 - val_acc: 0.9944\n",
      "Epoch 54/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0135 - val_acc: 0.9981\n",
      "Epoch 55/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0165 - val_acc: 0.9981\n",
      "Epoch 56/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0175 - val_acc: 0.9953\n",
      "Epoch 57/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0079 - acc: 0.9985 - val_loss: 0.0355 - val_acc: 0.9925\n",
      "Epoch 58/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0213 - val_acc: 0.9972\n",
      "Epoch 59/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0112 - val_acc: 0.9981\n",
      "Epoch 60/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0115 - val_acc: 0.9991\n",
      "Epoch 61/100\n",
      "20249/20249 [==============================] - 66s - loss: 6.6586e-04 - acc: 0.9998 - val_loss: 0.0163 - val_acc: 0.9981\n",
      "Epoch 62/100\n",
      "20249/20249 [==============================] - 66s - loss: 2.5742e-05 - acc: 1.0000 - val_loss: 0.0167 - val_acc: 0.9981\n",
      "Epoch 63/100\n",
      "20249/20249 [==============================] - 66s - loss: 2.7347e-04 - acc: 0.9999 - val_loss: 0.0158 - val_acc: 0.9991\n",
      "Epoch 64/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0165 - acc: 0.9961 - val_loss: 0.0374 - val_acc: 0.9944\n",
      "Epoch 65/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0183 - acc: 0.9958 - val_loss: 0.0168 - val_acc: 0.9972\n",
      "Epoch 66/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0037 - acc: 0.9991 - val_loss: 0.0143 - val_acc: 0.9981\n",
      "Epoch 67/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0121 - val_acc: 0.9991\n",
      "Epoch 68/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0245 - val_acc: 0.9972\n",
      "Epoch 69/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0168 - val_acc: 0.9981\n",
      "Epoch 70/100\n",
      "20249/20249 [==============================] - 66s - loss: 7.9347e-04 - acc: 0.9997 - val_loss: 0.0247 - val_acc: 0.9981\n",
      "Epoch 71/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0101 - acc: 0.9977 - val_loss: 0.0117 - val_acc: 0.9972\n",
      "Epoch 72/100\n",
      "20249/20249 [==============================] - 67s - loss: 4.0294e-04 - acc: 0.9999 - val_loss: 0.0090 - val_acc: 0.9972\n",
      "Epoch 73/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0066 - acc: 0.9985 - val_loss: 0.0096 - val_acc: 0.9991\n",
      "Epoch 74/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0090 - acc: 0.9978 - val_loss: 0.0217 - val_acc: 0.9981\n",
      "Epoch 75/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0110 - val_acc: 0.9991\n",
      "Epoch 76/100\n",
      "20249/20249 [==============================] - 66s - loss: 1.6905e-04 - acc: 1.0000 - val_loss: 0.0142 - val_acc: 0.9981\n",
      "Epoch 77/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0170 - val_acc: 0.9972\n",
      "Epoch 78/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0072 - acc: 0.9983 - val_loss: 0.0160 - val_acc: 0.9981\n",
      "Epoch 79/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0160 - val_acc: 0.9981\n",
      "Epoch 80/100\n",
      "20249/20249 [==============================] - 66s - loss: 5.2349e-04 - acc: 0.9998 - val_loss: 0.0154 - val_acc: 0.9991\n",
      "Epoch 81/100\n",
      "20249/20249 [==============================] - 66s - loss: 8.2246e-04 - acc: 0.9998 - val_loss: 0.0153 - val_acc: 0.9991\n",
      "Epoch 82/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0064 - acc: 0.9984 - val_loss: 0.1146 - val_acc: 0.9831\n",
      "Epoch 83/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0227 - val_acc: 0.9972\n",
      "Epoch 84/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0049 - acc: 0.9993 - val_loss: 0.0160 - val_acc: 0.9981\n",
      "Epoch 85/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0183 - val_acc: 0.9972\n",
      "Epoch 86/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0062 - acc: 0.9989 - val_loss: 0.0170 - val_acc: 0.9972\n",
      "Epoch 87/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0075 - acc: 0.9981 - val_loss: 0.0254 - val_acc: 0.9972\n",
      "Epoch 88/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0092 - acc: 0.9982 - val_loss: 0.0043 - val_acc: 0.9972\n",
      "Epoch 89/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0034 - val_acc: 0.9991\n",
      "Epoch 90/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0085 - val_acc: 0.9981\n",
      "Epoch 91/100\n",
      "20249/20249 [==============================] - 66s - loss: 4.5569e-05 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9981\n",
      "Epoch 92/100\n",
      "20249/20249 [==============================] - 67s - loss: 2.7808e-05 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9991\n",
      "Epoch 93/100\n",
      "20249/20249 [==============================] - 66s - loss: 3.6725e-05 - acc: 1.0000 - val_loss: 0.0101 - val_acc: 0.9981\n",
      "Epoch 94/100\n",
      "20249/20249 [==============================] - 66s - loss: 2.5862e-04 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9972\n",
      "Epoch 95/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0159 - acc: 0.9964 - val_loss: 0.0181 - val_acc: 0.9962\n",
      "Epoch 96/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0106 - acc: 0.9979 - val_loss: 0.0056 - val_acc: 0.9981\n",
      "Epoch 97/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0146 - acc: 0.9974 - val_loss: 0.0195 - val_acc: 0.9981\n",
      "Epoch 98/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0037 - acc: 0.9992 - val_loss: 0.0132 - val_acc: 0.9991\n",
      "Epoch 99/100\n",
      "20249/20249 [==============================] - 66s - loss: 8.5980e-04 - acc: 0.9997 - val_loss: 0.0167 - val_acc: 0.9981\n",
      "Epoch 100/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0138 - val_acc: 0.9981\n",
      "Model has accuracy: 99.910793934 %\n",
      "Name: 165_165_163_200_200\tKernelsize: (5, 5, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 71, 96, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 35, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 31, 44, 16)        6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 15, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 13, 20, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 10, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               192200    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 241,753\n",
      "Trainable params: 241,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Created path: ../Data/2017-09-16/165_165_163_200_200/\n",
      "Train on 20249 samples, validate on 1066 samples\n",
      "Epoch 1/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.5622 - acc: 0.7191 - val_loss: 0.4556 - val_acc: 0.7974\n",
      "Epoch 2/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.4440 - acc: 0.7867 - val_loss: 0.3398 - val_acc: 0.8265\n",
      "Epoch 3/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.3310 - acc: 0.8456 - val_loss: 0.2195 - val_acc: 0.9118\n",
      "Epoch 4/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.2512 - acc: 0.8881 - val_loss: 0.2405 - val_acc: 0.9081\n",
      "Epoch 5/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.1740 - acc: 0.9301 - val_loss: 0.1048 - val_acc: 0.9587\n",
      "Epoch 6/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.1139 - acc: 0.9544 - val_loss: 0.0702 - val_acc: 0.9728\n",
      "Epoch 7/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0817 - acc: 0.9690 - val_loss: 0.0657 - val_acc: 0.9747\n",
      "Epoch 8/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0589 - acc: 0.9787 - val_loss: 0.0549 - val_acc: 0.9775\n",
      "Epoch 9/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0436 - acc: 0.9838 - val_loss: 0.0775 - val_acc: 0.9784\n",
      "Epoch 10/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0369 - acc: 0.9872 - val_loss: 0.0257 - val_acc: 0.9934\n",
      "Epoch 11/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0333 - acc: 0.9881 - val_loss: 0.0335 - val_acc: 0.9916\n",
      "Epoch 12/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0293 - acc: 0.9895 - val_loss: 0.0144 - val_acc: 0.9991\n",
      "Epoch 13/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0229 - acc: 0.9927 - val_loss: 0.0394 - val_acc: 0.9878\n",
      "Epoch 14/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0220 - acc: 0.9917 - val_loss: 0.0268 - val_acc: 0.9925\n",
      "Epoch 15/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0153 - acc: 0.9942 - val_loss: 0.0227 - val_acc: 0.9925\n",
      "Epoch 16/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0146 - acc: 0.9948 - val_loss: 0.0212 - val_acc: 0.9916\n",
      "Epoch 17/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0160 - acc: 0.9945 - val_loss: 0.0464 - val_acc: 0.9850\n",
      "Epoch 18/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0180 - acc: 0.9942 - val_loss: 0.0199 - val_acc: 0.9953\n",
      "Epoch 19/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0101 - acc: 0.9960 - val_loss: 0.0207 - val_acc: 0.9953\n",
      "Epoch 20/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0069 - acc: 0.9973 - val_loss: 0.0129 - val_acc: 0.9981\n",
      "Epoch 21/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0176 - acc: 0.9947 - val_loss: 0.0412 - val_acc: 0.9859\n",
      "Epoch 22/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0107 - val_acc: 0.9991\n",
      "Epoch 23/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0245 - val_acc: 0.9925\n",
      "Epoch 24/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0142 - acc: 0.9960 - val_loss: 0.0151 - val_acc: 0.9962\n",
      "Epoch 25/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0071 - acc: 0.9977 - val_loss: 0.0183 - val_acc: 0.9953\n",
      "Epoch 26/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0112 - val_acc: 0.9972\n",
      "Epoch 27/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0183 - val_acc: 0.9925\n",
      "Epoch 28/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0078 - acc: 0.9977 - val_loss: 0.0120 - val_acc: 0.9972\n",
      "Epoch 29/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0084 - val_acc: 0.9972\n",
      "Epoch 30/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0119 - acc: 0.9961 - val_loss: 0.0094 - val_acc: 0.9953\n",
      "Epoch 31/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0093 - val_acc: 0.9972\n",
      "Epoch 32/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0043 - val_acc: 0.9981\n",
      "Epoch 33/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0153 - acc: 0.9957 - val_loss: 0.0148 - val_acc: 0.9944\n",
      "Epoch 34/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0120 - val_acc: 0.9981\n",
      "Epoch 35/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0104 - val_acc: 0.9981\n",
      "Epoch 36/100\n",
      "20249/20249 [==============================] - 69s - loss: 9.1342e-04 - acc: 0.9997 - val_loss: 0.0083 - val_acc: 0.9981\n",
      "Epoch 37/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0156 - acc: 0.9953 - val_loss: 0.0184 - val_acc: 0.9953\n",
      "Epoch 38/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0078 - acc: 0.9980 - val_loss: 0.0314 - val_acc: 0.9944\n",
      "Epoch 39/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0087 - acc: 0.9967 - val_loss: 0.0153 - val_acc: 0.9962\n",
      "Epoch 40/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0036 - val_acc: 0.9991\n",
      "Epoch 41/100\n",
      "20249/20249 [==============================] - 69s - loss: 3.8975e-04 - acc: 0.9999 - val_loss: 0.0073 - val_acc: 0.9981\n",
      "Epoch 42/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0110 - val_acc: 0.9972\n",
      "Epoch 43/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0050 - val_acc: 0.9981\n",
      "Epoch 44/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0111 - val_acc: 0.9981\n",
      "Epoch 45/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0216 - val_acc: 0.9944\n",
      "Epoch 46/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0240 - val_acc: 0.9944\n",
      "Epoch 47/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0054 - acc: 0.9987 - val_loss: 0.0053 - val_acc: 0.9981\n",
      "Epoch 48/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0064 - val_acc: 0.9981\n",
      "Epoch 49/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0160 - val_acc: 0.9944\n",
      "Epoch 50/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0110 - val_acc: 0.9972\n",
      "Epoch 51/100\n",
      "20249/20249 [==============================] - 69s - loss: 7.5481e-04 - acc: 0.9997 - val_loss: 0.0028 - val_acc: 0.9981\n",
      "Epoch 52/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0196 - acc: 0.9943 - val_loss: 0.0169 - val_acc: 0.9962\n",
      "Epoch 53/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0144 - val_acc: 0.9981\n",
      "Epoch 54/100\n",
      "20249/20249 [==============================] - 69s - loss: 5.8327e-04 - acc: 0.9999 - val_loss: 0.0134 - val_acc: 0.9991\n",
      "Epoch 55/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0230 - val_acc: 0.9972\n",
      "Epoch 56/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0124 - acc: 0.9970 - val_loss: 0.0078 - val_acc: 0.9981\n",
      "Epoch 57/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0100 - val_acc: 0.9972\n",
      "Epoch 58/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0055 - val_acc: 0.9991\n",
      "Epoch 59/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0067 - acc: 0.9981 - val_loss: 0.0138 - val_acc: 0.9981\n",
      "Epoch 60/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0127 - val_acc: 0.9981\n",
      "Epoch 61/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0063 - acc: 0.9986 - val_loss: 0.0041 - val_acc: 0.9991\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 69s - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0110 - val_acc: 0.9981\n",
      "Epoch 63/100\n",
      "20249/20249 [==============================] - 69s - loss: 5.4929e-04 - acc: 0.9998 - val_loss: 0.0144 - val_acc: 0.9962\n",
      "Epoch 64/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.8163e-04 - acc: 0.9999 - val_loss: 0.0110 - val_acc: 0.9981\n",
      "Epoch 65/100\n",
      "20249/20249 [==============================] - 69s - loss: 9.4014e-05 - acc: 1.0000 - val_loss: 0.0140 - val_acc: 0.9972\n",
      "Epoch 66/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0072 - acc: 0.9981 - val_loss: 0.0189 - val_acc: 0.9953\n",
      "Epoch 67/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0045 - val_acc: 0.9972\n",
      "Epoch 68/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0039 - acc: 0.9986 - val_loss: 0.0154 - val_acc: 0.9962\n",
      "Epoch 69/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0073 - val_acc: 0.9991\n",
      "Epoch 70/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0059 - val_acc: 0.9981\n",
      "Epoch 71/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0124 - val_acc: 0.9962\n",
      "Epoch 72/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0108 - val_acc: 0.9981\n",
      "Epoch 73/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0115 - acc: 0.9966 - val_loss: 0.0081 - val_acc: 0.9981\n",
      "Epoch 74/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0066 - val_acc: 0.9991\n",
      "Epoch 75/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0068 - acc: 0.9987 - val_loss: 0.0251 - val_acc: 0.9953\n",
      "Epoch 76/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0071 - val_acc: 0.9972\n",
      "Epoch 77/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0154 - val_acc: 0.9981\n",
      "Epoch 78/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0031 - val_acc: 0.9991\n",
      "Epoch 79/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0051 - val_acc: 0.9972\n",
      "Epoch 80/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0036 - acc: 0.9994 - val_loss: 0.0128 - val_acc: 0.9944\n",
      "Epoch 81/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0231 - val_acc: 0.9953\n",
      "Epoch 82/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0058 - acc: 0.9987 - val_loss: 0.0145 - val_acc: 0.9972\n",
      "Epoch 83/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0070 - acc: 0.9983 - val_loss: 0.0044 - val_acc: 0.9981\n",
      "Epoch 84/100\n",
      "20249/20249 [==============================] - 69s - loss: 9.2187e-04 - acc: 0.9997 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 85/100\n",
      "20249/20249 [==============================] - 69s - loss: 5.8211e-04 - acc: 0.9998 - val_loss: 0.0024 - val_acc: 0.9991\n",
      "Epoch 86/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0024 - val_acc: 0.9981\n",
      "Epoch 87/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0059 - val_acc: 0.9972\n",
      "Epoch 88/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0026 - val_acc: 0.9991\n",
      "Epoch 89/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0034 - val_acc: 0.9991\n",
      "Epoch 90/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0151 - val_acc: 0.9962\n",
      "Epoch 91/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0093 - acc: 0.9983 - val_loss: 0.0073 - val_acc: 0.9972\n",
      "Epoch 92/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0040 - acc: 0.9989 - val_loss: 0.0082 - val_acc: 0.9981\n",
      "Epoch 93/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0227 - val_acc: 0.9953\n",
      "Epoch 94/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0121 - val_acc: 0.9962\n",
      "Epoch 95/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0048 - acc: 0.9991 - val_loss: 0.0098 - val_acc: 0.9991\n",
      "Epoch 96/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0031 - acc: 0.9995 - val_loss: 0.0030 - val_acc: 0.9981\n",
      "Epoch 97/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.7729e-04 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 0.9981\n",
      "Epoch 98/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0234 - val_acc: 0.9962\n",
      "Epoch 99/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0522 - val_acc: 0.9906\n",
      "Epoch 100/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0103 - acc: 0.9978 - val_loss: 0.0115 - val_acc: 0.9972\n",
      "Model has accuracy: 99.4647636039 %\n",
      "Name: 165_163_163_200_200\tKernelsize: (5, 3, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 71, 96, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 35, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 33, 46, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 23, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 21, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 7, 10, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1120)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 200)               224200    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 269,657\n",
      "Trainable params: 269,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Created path: ../Data/2017-09-16/165_163_163_200_200/\n",
      "Train on 20249 samples, validate on 1066 samples\n",
      "Epoch 1/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.5619 - acc: 0.7197 - val_loss: 0.4207 - val_acc: 0.8039\n",
      "Epoch 2/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.4223 - acc: 0.8059 - val_loss: 0.3850 - val_acc: 0.8321\n",
      "Epoch 3/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.3153 - acc: 0.8614 - val_loss: 0.2143 - val_acc: 0.9053\n",
      "Epoch 4/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.1653 - acc: 0.9316 - val_loss: 0.0950 - val_acc: 0.9644\n",
      "Epoch 5/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0972 - acc: 0.9613 - val_loss: 0.0511 - val_acc: 0.9859\n",
      "Epoch 6/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0595 - acc: 0.9780 - val_loss: 0.0605 - val_acc: 0.9784\n",
      "Epoch 7/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0418 - acc: 0.9841 - val_loss: 0.0451 - val_acc: 0.9887\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 69s - loss: 0.0250 - acc: 0.9915 - val_loss: 0.0276 - val_acc: 0.9925\n",
      "Epoch 9/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0228 - acc: 0.9915 - val_loss: 0.0355 - val_acc: 0.9916\n",
      "Epoch 10/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0139 - acc: 0.9952 - val_loss: 0.0426 - val_acc: 0.9925\n",
      "Epoch 11/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0210 - acc: 0.9922 - val_loss: 0.0279 - val_acc: 0.9944\n",
      "Epoch 12/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0112 - acc: 0.9963 - val_loss: 0.0315 - val_acc: 0.9962\n",
      "Epoch 13/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0093 - acc: 0.9966 - val_loss: 0.0418 - val_acc: 0.9944\n",
      "Epoch 14/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0146 - acc: 0.9952 - val_loss: 0.0509 - val_acc: 0.9887\n",
      "Epoch 15/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0284 - val_acc: 0.9962\n",
      "Epoch 16/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0088 - acc: 0.9969 - val_loss: 0.0342 - val_acc: 0.9953\n",
      "Epoch 17/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0348 - val_acc: 0.9962\n",
      "Epoch 18/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0083 - acc: 0.9970 - val_loss: 0.0439 - val_acc: 0.9916\n",
      "Epoch 19/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0267 - val_acc: 0.9972\n",
      "Epoch 20/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0260 - val_acc: 0.9962\n",
      "Epoch 21/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0135 - acc: 0.9952 - val_loss: 0.0205 - val_acc: 0.9962\n",
      "Epoch 22/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0211 - val_acc: 0.9972\n",
      "Epoch 23/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0439 - val_acc: 0.9916\n",
      "Epoch 24/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0151 - acc: 0.9954 - val_loss: 0.0181 - val_acc: 0.9972\n",
      "Epoch 25/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0197 - val_acc: 0.9972\n",
      "Epoch 26/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0179 - val_acc: 0.9972\n",
      "Epoch 27/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0245 - val_acc: 0.9953\n",
      "Epoch 28/100\n",
      "20249/20249 [==============================] - 69s - loss: 4.2271e-04 - acc: 0.9999 - val_loss: 0.0242 - val_acc: 0.9944\n",
      "Epoch 29/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0069 - acc: 0.9978 - val_loss: 0.0230 - val_acc: 0.9916\n",
      "Epoch 30/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0147 - acc: 0.9960 - val_loss: 0.0253 - val_acc: 0.9934\n",
      "Epoch 31/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0356 - val_acc: 0.9934\n",
      "Epoch 32/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0258 - val_acc: 0.9972\n",
      "Epoch 33/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.5821e-04 - acc: 1.0000 - val_loss: 0.0332 - val_acc: 0.9962\n",
      "Epoch 34/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0078 - acc: 0.9977 - val_loss: 0.0327 - val_acc: 0.9925\n",
      "Epoch 35/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0111 - acc: 0.9967 - val_loss: 0.0237 - val_acc: 0.9972\n",
      "Epoch 36/100\n",
      "20249/20249 [==============================] - 68s - loss: 3.3519e-04 - acc: 0.9999 - val_loss: 0.0296 - val_acc: 0.9962\n",
      "Epoch 37/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0045 - acc: 0.9988 - val_loss: 0.0289 - val_acc: 0.9925\n",
      "Epoch 38/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0216 - val_acc: 0.9962\n",
      "Epoch 39/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0077 - acc: 0.9977 - val_loss: 0.0271 - val_acc: 0.9953\n",
      "Epoch 40/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0148 - val_acc: 0.9972\n",
      "Epoch 41/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0142 - val_acc: 0.9962\n",
      "Epoch 42/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0342 - val_acc: 0.9934\n",
      "Epoch 43/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0107 - acc: 0.9973 - val_loss: 0.0360 - val_acc: 0.9972\n",
      "Epoch 44/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0230 - val_acc: 0.9981\n",
      "Epoch 45/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0239 - val_acc: 0.9981\n",
      "Epoch 46/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0163 - val_acc: 0.9981\n",
      "Epoch 47/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0170 - val_acc: 0.9981\n",
      "Epoch 48/100\n",
      "20249/20249 [==============================] - 68s - loss: 5.4122e-04 - acc: 0.9998 - val_loss: 0.0233 - val_acc: 0.9981\n",
      "Epoch 49/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0021 - acc: 0.9994 - val_loss: 0.0114 - val_acc: 0.9981\n",
      "Epoch 50/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0152 - val_acc: 0.9981\n",
      "Epoch 51/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0267 - val_acc: 0.9953\n",
      "Epoch 52/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0082 - val_acc: 0.9991\n",
      "Epoch 53/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0135 - val_acc: 0.9981\n",
      "Epoch 54/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0144 - val_acc: 0.9962\n",
      "Epoch 55/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.2572e-04 - acc: 1.0000 - val_loss: 0.0167 - val_acc: 0.9981\n",
      "Epoch 56/100\n",
      "20249/20249 [==============================] - 69s - loss: 5.9531e-04 - acc: 0.9998 - val_loss: 0.0602 - val_acc: 0.9906\n",
      "Epoch 57/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0142 - acc: 0.9960 - val_loss: 0.0178 - val_acc: 0.9972\n",
      "Epoch 58/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0171 - val_acc: 0.9972\n",
      "Epoch 59/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0169 - val_acc: 0.9981\n",
      "Epoch 60/100\n",
      "20249/20249 [==============================] - 68s - loss: 6.5771e-04 - acc: 0.9999 - val_loss: 0.0218 - val_acc: 0.9981\n",
      "Epoch 61/100\n",
      "20249/20249 [==============================] - 69s - loss: 2.4458e-04 - acc: 1.0000 - val_loss: 0.0211 - val_acc: 0.9972\n",
      "Epoch 62/100\n",
      "20249/20249 [==============================] - 68s - loss: 7.7431e-04 - acc: 0.9997 - val_loss: 0.0326 - val_acc: 0.9972\n",
      "Epoch 63/100\n",
      "20249/20249 [==============================] - 68s - loss: 9.3275e-04 - acc: 0.9998 - val_loss: 0.0193 - val_acc: 0.9972\n",
      "Epoch 64/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0110 - acc: 0.9974 - val_loss: 0.0241 - val_acc: 0.9972\n",
      "Epoch 65/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0215 - val_acc: 0.9972\n",
      "Epoch 66/100\n",
      "20249/20249 [==============================] - 68s - loss: 6.6071e-04 - acc: 0.9999 - val_loss: 0.0190 - val_acc: 0.9981\n",
      "Epoch 67/100\n",
      "20249/20249 [==============================] - 69s - loss: 6.2189e-05 - acc: 1.0000 - val_loss: 0.0181 - val_acc: 0.9981\n",
      "Epoch 68/100\n",
      "20249/20249 [==============================] - 68s - loss: 5.2345e-05 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 0.9981\n",
      "Epoch 69/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0118 - val_acc: 0.9981\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 68s - loss: 1.5368e-04 - acc: 1.0000 - val_loss: 0.0291 - val_acc: 0.9962\n",
      "Epoch 71/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0160 - val_acc: 0.9991\n",
      "Epoch 72/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0091 - val_acc: 0.9972\n",
      "Epoch 73/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0097 - val_acc: 0.9981\n",
      "Epoch 74/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0201 - val_acc: 0.9981\n",
      "Epoch 75/100\n",
      "20249/20249 [==============================] - 68s - loss: 9.1820e-04 - acc: 0.9997 - val_loss: 0.0043 - val_acc: 0.9981\n",
      "Epoch 76/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0010 - acc: 0.9998 - val_loss: 0.0065 - val_acc: 0.9981\n",
      "Epoch 77/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0012 - acc: 0.9995 - val_loss: 0.0131 - val_acc: 0.9972\n",
      "Epoch 78/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0057 - acc: 0.9990 - val_loss: 0.0036 - val_acc: 0.9972\n",
      "Epoch 79/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0069 - val_acc: 0.9991\n",
      "Epoch 80/100\n",
      "20249/20249 [==============================] - 68s - loss: 9.2565e-05 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9991\n",
      "Epoch 81/100\n",
      "20249/20249 [==============================] - 68s - loss: 1.3958e-04 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 0.9962\n",
      "Epoch 82/100\n",
      "20249/20249 [==============================] - 68s - loss: 2.7883e-05 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Epoch 83/100\n",
      "20249/20249 [==============================] - 68s - loss: 2.8358e-05 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9991\n",
      "Epoch 84/100\n",
      "20249/20249 [==============================] - 69s - loss: 3.7962e-06 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9991\n",
      "Epoch 85/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0169 - acc: 0.9964 - val_loss: 0.0073 - val_acc: 0.9981\n",
      "Epoch 86/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0012 - acc: 0.9998 - val_loss: 0.0107 - val_acc: 0.9981\n",
      "Epoch 87/100\n",
      "20249/20249 [==============================] - 68s - loss: 1.5403e-04 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9981\n",
      "Epoch 88/100\n",
      "20249/20249 [==============================] - 68s - loss: 9.3894e-05 - acc: 1.0000 - val_loss: 0.0174 - val_acc: 0.9981\n",
      "Epoch 89/100\n",
      "20249/20249 [==============================] - 68s - loss: 3.2995e-05 - acc: 1.0000 - val_loss: 0.0166 - val_acc: 0.9981\n",
      "Epoch 90/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.9457e-05 - acc: 1.0000 - val_loss: 0.0175 - val_acc: 0.9981\n",
      "Epoch 91/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.4879e-05 - acc: 1.0000 - val_loss: 0.0168 - val_acc: 0.9981\n",
      "Epoch 92/100\n",
      "20249/20249 [==============================] - 68s - loss: 1.6076e-05 - acc: 1.0000 - val_loss: 0.0238 - val_acc: 0.9972\n",
      "Epoch 93/100\n",
      "20249/20249 [==============================] - 68s - loss: 2.1574e-05 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9972\n",
      "Epoch 94/100\n",
      "20249/20249 [==============================] - 68s - loss: 7.8809e-05 - acc: 1.0000 - val_loss: 0.0254 - val_acc: 0.9981\n",
      "Epoch 95/100\n",
      "20249/20249 [==============================] - 68s - loss: 1.1774e-05 - acc: 1.0000 - val_loss: 0.0267 - val_acc: 0.9972\n",
      "Epoch 96/100\n",
      "20249/20249 [==============================] - 68s - loss: 2.0171e-05 - acc: 1.0000 - val_loss: 0.0240 - val_acc: 0.9981\n",
      "Epoch 97/100\n",
      "20249/20249 [==============================] - 68s - loss: 2.5759e-05 - acc: 1.0000 - val_loss: 0.0143 - val_acc: 0.9981\n",
      "Epoch 98/100\n",
      "20249/20249 [==============================] - 68s - loss: 7.3434e-06 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9991\n",
      "Epoch 99/100\n",
      "20249/20249 [==============================] - 68s - loss: 7.3221e-06 - acc: 1.0000 - val_loss: 0.0159 - val_acc: 0.9981\n",
      "Epoch 100/100\n",
      "20249/20249 [==============================] - 69s - loss: 3.5557e-06 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9981\n",
      "Model has accuracy: 99.910793934 %\n"
     ]
    }
   ],
   "source": [
    "for name, k_size in zip(names_3Conv_2Fully, kernel_sizes_3Conv_2Fully):    \n",
    "    print('Name: {}\\tKernelsize: {}'.format(name, k_size))\n",
    "    #print('k_sizes: {},{},{}'.format(k_size[0], k_size[1], k_size[2]))\n",
    "    \n",
    "    ## Path for Model saving!\n",
    "    path_model = '../TrainedModels/' + str(datetime.now().strftime('%Y-%m-%d')) + '/'\n",
    "\n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "        print('Created path: {}'.format(path_model))\n",
    "        \n",
    "    ##### MODEL #####\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional layer initialised with random input weights\n",
    "    model.add(Conv2D(16, (k_size[0], k_size[0]), kernel_initializer=RandomNormal(mean=0, stddev=1/(h*w)), padding='valid', input_shape=input_shape, activation='selu'))\n",
    "    # Reduce size a bit\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    mult_shape1 = np.prod(model.layers[1].output_shape[1:])\n",
    "\n",
    "    # Second Convolutional layer\n",
    "    model.add(Conv2D(16, (k_size[1], k_size[1]), kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape1), padding='valid', activation='selu'))\n",
    "    # Reduce size a bit\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    mult_shape2 = np.prod(model.layers[3].output_shape[1:])\n",
    "\n",
    "    # Third Convolutional layer\n",
    "    model.add(Conv2D(16, (k_size[2], k_size[2]), kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape2), padding='valid', activation='selu'))\n",
    "    # Reduce size a bit\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    #Converting the 2D images to 1D vectors\n",
    "    model.add(Flatten())  \n",
    "    mult_shape3 = np.prod(model.layers[6].output_shape[1:])\n",
    "\n",
    "    # First Fully connected layer\n",
    "    model.add(Dense(200, activation='selu', kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    mult_shape4 = np.prod(model.layers[8].output_shape[1:])\n",
    "\n",
    "    # Second Fully connected layer\n",
    "    model.add(Dense(200, activation='selu', kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape4)))\n",
    "    model.add(Dropout(0.2))\n",
    "    mult_shape5 = np.prod(model.layers[10].output_shape[1:])\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape5)))\n",
    "    \n",
    "    #### END OF MODEL ####\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    ## Save the Model and create Data path!\n",
    "    path_data = '../Data/{}/{}/'.format(str(datetime.now().strftime('%Y-%m-%d')), name )\n",
    "\n",
    "    if not os.path.exists(path_data):\n",
    "        os.makedirs(path_data)\n",
    "        print('Created path: {}'.format(path_data))\n",
    "\n",
    "    # Open the file\n",
    "    with open(path_data + 'ModelSummary.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "        \n",
    "    \n",
    "    #Defining optimiser and compiling the model\n",
    "    model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "                  optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = LossHistory()\n",
    "    \n",
    "    ## Start the training!\n",
    "    model.fit(X_train_norm, Y_train,  batch_size=76, epochs=100, verbose=1, validation_split = 0.05, callbacks=[history])\n",
    "\n",
    "    #Evaluating trained model on test images\n",
    "    score = model.evaluate(X_test_norm, Y_test, verbose=0)\n",
    "    print('Model has accuracy:', score[1]*100,'%')\n",
    "    \n",
    "    # Save trained NN\n",
    "    model.save(path_model + name + '.h5')\n",
    "    np.savetxt(path_model + name + '_TrainingData.txt', np.transpose([history.accuracy, history.losses]))\n",
    "    \n",
    "    ### Performance on Testdata! ###\n",
    "    # Get probs per image\n",
    "    probs = []\n",
    "\n",
    "    i=0 \n",
    "    for img in X_test_norm:\n",
    "        #plt.imshow(np.squeeze(img), cmap='gray')\n",
    "\n",
    "        img = np.reshape(img, [1,h,w,1])\n",
    "        p = model.predict_proba(img, verbose=0)\n",
    "        probs.append(np.squeeze(p))\n",
    "\n",
    "        #name = \"Good img\" if Y_test[i, 1] == 1 else \"Bad img\"\n",
    "        #plt.title('{}. Score: [{:.4f}, {:.4f}]'.format(name, p[0,0], p[0,1]))\n",
    "        #plt.savefig('../Plots/TestImg/{}.png'.format(i))\n",
    "        i+=1\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    \n",
    "    np.savetxt(path_data + 'ScoresOnTestData.txt', np.transpose([Y_test, probs]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with 3 Conv Layer and 1 Fully Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_3Conv_1Fully = ['167_165_163_200', '165_165_163_200', '165_163_163_200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_sizes_3Conv_1Fully = [(7,5,3), (5,5,3), (5,3,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 167_165_163_200\tKernelsize: (7, 5, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 69, 94, 16)        800       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 34, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 30, 43, 16)        6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 15, 21, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 13, 19, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 9, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 864)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               173000    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 182,737\n",
      "Trainable params: 182,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Created path: ../Data/2017-09-16/167_165_163_200/\n",
      "Train on 20249 samples, validate on 1066 samples\n",
      "Epoch 1/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.5922 - acc: 0.6881 - val_loss: 0.5798 - val_acc: 0.7298\n",
      "Epoch 2/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.4521 - acc: 0.7865 - val_loss: 0.3617 - val_acc: 0.8330\n",
      "Epoch 3/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.3522 - acc: 0.8420 - val_loss: 0.2728 - val_acc: 0.8809\n",
      "Epoch 4/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.2699 - acc: 0.8881 - val_loss: 0.1969 - val_acc: 0.9174\n",
      "Epoch 5/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.1903 - acc: 0.9269 - val_loss: 0.1589 - val_acc: 0.9418\n",
      "Epoch 6/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.1122 - acc: 0.9597 - val_loss: 0.0721 - val_acc: 0.9747\n",
      "Epoch 7/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0654 - acc: 0.9775 - val_loss: 0.0397 - val_acc: 0.9906\n",
      "Epoch 8/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0512 - acc: 0.9825 - val_loss: 0.0600 - val_acc: 0.9822\n",
      "Epoch 9/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0368 - acc: 0.9870 - val_loss: 0.0732 - val_acc: 0.9719\n",
      "Epoch 10/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0337 - acc: 0.9881 - val_loss: 0.0460 - val_acc: 0.9831\n",
      "Epoch 11/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0215 - acc: 0.9932 - val_loss: 0.0238 - val_acc: 0.9925\n",
      "Epoch 12/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0174 - acc: 0.9935 - val_loss: 0.0312 - val_acc: 0.9925\n",
      "Epoch 13/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0154 - acc: 0.9952 - val_loss: 0.0123 - val_acc: 0.9972\n",
      "Epoch 14/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0141 - val_acc: 0.9953\n",
      "Epoch 15/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0280 - val_acc: 0.9944\n",
      "Epoch 16/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0102 - val_acc: 0.9953\n",
      "Epoch 17/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0143 - acc: 0.9953 - val_loss: 0.0083 - val_acc: 0.9962\n",
      "Epoch 18/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0135 - val_acc: 0.9962\n",
      "Epoch 19/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0088 - acc: 0.9967 - val_loss: 0.0092 - val_acc: 0.9981\n",
      "Epoch 20/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0094 - acc: 0.9969 - val_loss: 0.0155 - val_acc: 0.9953\n",
      "Epoch 21/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0062 - acc: 0.9977 - val_loss: 0.0292 - val_acc: 0.9906\n",
      "Epoch 22/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0094 - acc: 0.9971 - val_loss: 0.0651 - val_acc: 0.9822\n",
      "Epoch 23/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0086 - acc: 0.9968 - val_loss: 0.0033 - val_acc: 0.9981\n",
      "Epoch 24/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0060 - acc: 0.9977 - val_loss: 0.0369 - val_acc: 0.9906\n",
      "Epoch 25/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0151 - val_acc: 0.9944\n",
      "Epoch 26/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0047 - val_acc: 0.9981\n",
      "Epoch 27/100\n",
      "20249/20249 [==============================] - 66s - loss: 4.2622e-04 - acc: 0.9999 - val_loss: 0.0043 - val_acc: 0.9981\n",
      "Epoch 28/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0049 - val_acc: 0.9991\n",
      "Epoch 29/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0206 - val_acc: 0.9962\n",
      "Epoch 30/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0082 - acc: 0.9971 - val_loss: 0.0127 - val_acc: 0.9953\n",
      "Epoch 31/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0061 - val_acc: 0.9972\n",
      "Epoch 32/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0112 - val_acc: 0.9981\n",
      "Epoch 33/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0052 - val_acc: 0.9981\n",
      "Epoch 34/100\n",
      "20249/20249 [==============================] - 65s - loss: 8.3205e-05 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9981\n",
      "Epoch 35/100\n",
      "20249/20249 [==============================] - 65s - loss: 6.3367e-05 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9981\n",
      "Epoch 36/100\n",
      "20249/20249 [==============================] - 66s - loss: 8.0372e-05 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 0.9981\n",
      "Epoch 37/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0091 - acc: 0.9974 - val_loss: 0.0245 - val_acc: 0.9916\n",
      "Epoch 38/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0234 - acc: 0.9941 - val_loss: 0.0143 - val_acc: 0.9972\n",
      "Epoch 39/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0075 - val_acc: 0.9991\n",
      "Epoch 40/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0035 - val_acc: 0.9991\n",
      "Epoch 41/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0072 - val_acc: 0.9991\n",
      "Epoch 42/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0072 - val_acc: 0.9972\n",
      "Epoch 43/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0099 - val_acc: 0.9981\n",
      "Epoch 44/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0057 - acc: 0.9985 - val_loss: 0.0126 - val_acc: 0.9981\n",
      "Epoch 45/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0076 - val_acc: 0.9981\n",
      "Epoch 46/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0018 - acc: 0.9993 - val_loss: 0.0423 - val_acc: 0.9916\n",
      "Epoch 47/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0042 - acc: 0.9985 - val_loss: 0.0091 - val_acc: 0.9991\n",
      "Epoch 48/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0049 - acc: 0.9988 - val_loss: 0.0561 - val_acc: 0.9934\n",
      "Epoch 49/100\n",
      "20249/20249 [==============================] - 65s - loss: 0.0087 - acc: 0.9979 - val_loss: 0.0103 - val_acc: 0.9981\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 66s - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0385 - val_acc: 0.9916\n",
      "Epoch 51/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0093 - val_acc: 0.9972\n",
      "Epoch 52/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0100 - val_acc: 0.9981\n",
      "Epoch 53/100\n",
      "20249/20249 [==============================] - 65s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0033 - val_acc: 0.9981\n",
      "Epoch 54/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0160 - val_acc: 0.9972\n",
      "Epoch 55/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0048 - acc: 0.9990 - val_loss: 0.0203 - val_acc: 0.9953\n",
      "Epoch 56/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0126 - acc: 0.9968 - val_loss: 0.0191 - val_acc: 0.9981\n",
      "Epoch 57/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0115 - val_acc: 0.9981\n",
      "Epoch 58/100\n",
      "20249/20249 [==============================] - 66s - loss: 4.3703e-04 - acc: 0.9999 - val_loss: 0.0157 - val_acc: 0.9991\n",
      "Epoch 59/100\n",
      "20249/20249 [==============================] - 66s - loss: 1.9336e-04 - acc: 1.0000 - val_loss: 0.0189 - val_acc: 0.9981\n",
      "Epoch 60/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0184 - val_acc: 0.9972\n",
      "Epoch 61/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0189 - val_acc: 0.9972\n",
      "Epoch 62/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0157 - val_acc: 0.9991\n",
      "Epoch 63/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0299 - val_acc: 0.9953\n",
      "Epoch 64/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0198 - acc: 0.9964 - val_loss: 0.0266 - val_acc: 0.9962\n",
      "Epoch 65/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0079 - acc: 0.9978 - val_loss: 0.0101 - val_acc: 0.9981\n",
      "Epoch 66/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0056 - val_acc: 0.9991\n",
      "Epoch 67/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0025 - val_acc: 0.9991\n",
      "Epoch 68/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0044 - acc: 0.9990 - val_loss: 0.0023 - val_acc: 0.9981\n",
      "Epoch 69/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0156 - val_acc: 0.9991\n",
      "Epoch 70/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0132 - val_acc: 0.9991\n",
      "Epoch 71/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0034 - val_acc: 0.9991\n",
      "Epoch 72/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0035 - acc: 0.9993 - val_loss: 0.0165 - val_acc: 0.9972\n",
      "Epoch 73/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0083 - val_acc: 0.9991\n",
      "Epoch 74/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0130 - acc: 0.9976 - val_loss: 9.9740e-04 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0051 - acc: 0.9991 - val_loss: 0.0074 - val_acc: 0.9991\n",
      "Epoch 76/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0202 - val_acc: 0.9981\n",
      "Epoch 77/100\n",
      "20249/20249 [==============================] - 66s - loss: 2.7058e-04 - acc: 0.9999 - val_loss: 0.0359 - val_acc: 0.9953\n",
      "Epoch 78/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0174 - val_acc: 0.9981\n",
      "Epoch 79/100\n",
      "20249/20249 [==============================] - 66s - loss: 4.3663e-05 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 0.9991\n",
      "Epoch 80/100\n",
      "20249/20249 [==============================] - 66s - loss: 3.0085e-04 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 0.9981\n",
      "Epoch 81/100\n",
      "20249/20249 [==============================] - 66s - loss: 2.1259e-04 - acc: 0.9999 - val_loss: 0.0192 - val_acc: 0.9981\n",
      "Epoch 82/100\n",
      "20249/20249 [==============================] - 66s - loss: 2.0758e-05 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9981\n",
      "Epoch 83/100\n",
      "20249/20249 [==============================] - 66s - loss: 1.3942e-05 - acc: 1.0000 - val_loss: 0.0198 - val_acc: 0.9981\n",
      "Epoch 84/100\n",
      "20249/20249 [==============================] - 66s - loss: 1.0529e-05 - acc: 1.0000 - val_loss: 0.0141 - val_acc: 0.9991\n",
      "Epoch 85/100\n",
      "20249/20249 [==============================] - 66s - loss: 4.3994e-07 - acc: 1.0000 - val_loss: 0.0149 - val_acc: 0.9991\n",
      "Epoch 86/100\n",
      "20249/20249 [==============================] - 66s - loss: 1.1776e-05 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9991\n",
      "Epoch 87/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0055 - acc: 0.9989 - val_loss: 0.5588 - val_acc: 0.9456\n",
      "Epoch 88/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0322 - acc: 0.9951 - val_loss: 0.0199 - val_acc: 0.9981\n",
      "Epoch 89/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0073 - acc: 0.9989 - val_loss: 0.0208 - val_acc: 0.9962\n",
      "Epoch 90/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0012 - val_acc: 0.9991\n",
      "Epoch 91/100\n",
      "20249/20249 [==============================] - 66s - loss: 3.5772e-04 - acc: 0.9999 - val_loss: 0.0013 - val_acc: 0.9991\n",
      "Epoch 92/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0132 - val_acc: 0.9972\n",
      "Epoch 93/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0044 - val_acc: 0.9981\n",
      "Epoch 94/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0112 - acc: 0.9984 - val_loss: 0.0180 - val_acc: 0.9972\n",
      "Epoch 95/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0682 - val_acc: 0.9925\n",
      "Epoch 96/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0140 - acc: 0.9977 - val_loss: 0.0224 - val_acc: 0.9972\n",
      "Epoch 97/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0162 - val_acc: 0.9981\n",
      "Epoch 98/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0069 - acc: 0.9988 - val_loss: 0.0288 - val_acc: 0.9953\n",
      "Epoch 99/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0298 - val_acc: 0.9962\n",
      "Epoch 100/100\n",
      "20249/20249 [==============================] - 66s - loss: 5.5494e-04 - acc: 0.9999 - val_loss: 0.0429 - val_acc: 0.9962\n",
      "Model has accuracy: 99.910793934 %\n",
      "Name: 165_165_163_200\tKernelsize: (5, 5, 3)\n",
      "Created path: ../TrainedModels/2017-09-17/\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 71, 96, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 35, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 31, 44, 16)        6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 15, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 13, 20, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 6, 10, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               192200    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 201,553\n",
      "Trainable params: 201,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Created path: ../Data/2017-09-17/165_165_163_200/\n",
      "Train on 20249 samples, validate on 1066 samples\n",
      "Epoch 1/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.5924 - acc: 0.6902 - val_loss: 0.5404 - val_acc: 0.7373\n",
      "Epoch 2/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.4646 - acc: 0.7749 - val_loss: 0.3852 - val_acc: 0.8171\n",
      "Epoch 3/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.3879 - acc: 0.8228 - val_loss: 0.2979 - val_acc: 0.8818\n",
      "Epoch 4/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.3159 - acc: 0.8625 - val_loss: 0.2232 - val_acc: 0.9053\n",
      "Epoch 5/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.2473 - acc: 0.8955 - val_loss: 0.1666 - val_acc: 0.9371\n",
      "Epoch 6/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.1668 - acc: 0.9332 - val_loss: 0.0950 - val_acc: 0.9644\n",
      "Epoch 7/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.1070 - acc: 0.9590 - val_loss: 0.0603 - val_acc: 0.9775\n",
      "Epoch 8/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0657 - acc: 0.9752 - val_loss: 0.0504 - val_acc: 0.9841\n",
      "Epoch 9/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0554 - acc: 0.9796 - val_loss: 0.0430 - val_acc: 0.9859\n",
      "Epoch 10/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0344 - acc: 0.9882 - val_loss: 0.0438 - val_acc: 0.9887\n",
      "Epoch 11/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0298 - acc: 0.9892 - val_loss: 0.0286 - val_acc: 0.9887\n",
      "Epoch 12/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0172 - acc: 0.9940 - val_loss: 0.0385 - val_acc: 0.9859\n",
      "Epoch 13/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0213 - acc: 0.9923 - val_loss: 0.0237 - val_acc: 0.9925\n",
      "Epoch 14/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0147 - acc: 0.9946 - val_loss: 0.0140 - val_acc: 0.9953\n",
      "Epoch 15/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0123 - acc: 0.9960 - val_loss: 0.0222 - val_acc: 0.9925\n",
      "Epoch 16/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0141 - acc: 0.9949 - val_loss: 0.0205 - val_acc: 0.9962\n",
      "Epoch 17/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0178 - val_acc: 0.9962\n",
      "Epoch 18/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0199 - val_acc: 0.9944\n",
      "Epoch 19/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0173 - acc: 0.9936 - val_loss: 0.0536 - val_acc: 0.9887\n",
      "Epoch 20/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0178 - acc: 0.9943 - val_loss: 0.0350 - val_acc: 0.9944\n",
      "Epoch 21/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0047 - acc: 0.9981 - val_loss: 0.0330 - val_acc: 0.9944\n",
      "Epoch 22/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0174 - val_acc: 0.9972\n",
      "Epoch 23/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0180 - val_acc: 0.9981\n",
      "Epoch 24/100\n",
      "20249/20249 [==============================] - 70s - loss: 6.2776e-04 - acc: 1.0000 - val_loss: 0.0210 - val_acc: 0.9972\n",
      "Epoch 25/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0266 - val_acc: 0.9925\n",
      "Epoch 26/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0179 - acc: 0.9940 - val_loss: 0.0243 - val_acc: 0.9953\n",
      "Epoch 27/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0127 - val_acc: 0.9981\n",
      "Epoch 28/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0201 - val_acc: 0.9972\n",
      "Epoch 29/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0162 - val_acc: 0.9981\n",
      "Epoch 30/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0222 - val_acc: 0.9972\n",
      "Epoch 31/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0076 - acc: 0.9972 - val_loss: 0.0191 - val_acc: 0.9953\n",
      "Epoch 32/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0193 - val_acc: 0.9981\n",
      "Epoch 33/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0346 - val_acc: 0.9944\n",
      "Epoch 34/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0263 - val_acc: 0.9934\n",
      "Epoch 35/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0045 - acc: 0.9982 - val_loss: 0.0450 - val_acc: 0.9887\n",
      "Epoch 36/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0109 - acc: 0.9968 - val_loss: 0.0172 - val_acc: 0.9981\n",
      "Epoch 37/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0115 - val_acc: 0.9981\n",
      "Epoch 38/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0295 - val_acc: 0.9953\n",
      "Epoch 39/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0071 - val_acc: 0.9981\n",
      "Epoch 40/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0244 - val_acc: 0.9916\n",
      "Epoch 41/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0231 - val_acc: 0.9962\n",
      "Epoch 42/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0042 - acc: 0.9985 - val_loss: 0.0234 - val_acc: 0.9953\n",
      "Epoch 43/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0255 - val_acc: 0.9953\n",
      "Epoch 44/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0084 - acc: 0.9979 - val_loss: 0.0071 - val_acc: 0.9981\n",
      "Epoch 45/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0230 - val_acc: 0.9962\n",
      "Epoch 46/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0150 - val_acc: 0.9962\n",
      "Epoch 47/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0283 - val_acc: 0.9962\n",
      "Epoch 48/100\n",
      "20249/20249 [==============================] - 70s - loss: 1.8319e-04 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 0.9981\n",
      "Epoch 49/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0203 - val_acc: 0.9962\n",
      "Epoch 50/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0309 - val_acc: 0.9934\n",
      "Epoch 51/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0243 - acc: 0.9952 - val_loss: 0.0250 - val_acc: 0.9972\n",
      "Epoch 52/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0152 - val_acc: 0.9972\n",
      "Epoch 53/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0150 - val_acc: 0.9972\n",
      "Epoch 54/100\n",
      "20249/20249 [==============================] - 70s - loss: 4.4909e-04 - acc: 0.9998 - val_loss: 0.0039 - val_acc: 0.9991\n",
      "Epoch 55/100\n",
      "20249/20249 [==============================] - 70s - loss: 8.8690e-04 - acc: 0.9998 - val_loss: 0.0247 - val_acc: 0.9972\n",
      "Epoch 56/100\n",
      "20249/20249 [==============================] - 69s - loss: 6.6504e-04 - acc: 0.9998 - val_loss: 0.0138 - val_acc: 0.9991\n",
      "Epoch 57/100\n",
      "20249/20249 [==============================] - 69s - loss: 2.1245e-04 - acc: 0.9999 - val_loss: 0.0116 - val_acc: 0.9981\n",
      "Epoch 58/100\n",
      "20249/20249 [==============================] - 69s - loss: 3.2718e-04 - acc: 0.9999 - val_loss: 0.0070 - val_acc: 0.9981\n",
      "Epoch 59/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0045 - acc: 0.9990 - val_loss: 0.0170 - val_acc: 0.9972\n",
      "Epoch 60/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0146 - val_acc: 0.9981\n",
      "Epoch 61/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0037 - val_acc: 0.9991\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 69s - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0164 - val_acc: 0.9972\n",
      "Epoch 63/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0056 - acc: 0.9985 - val_loss: 0.0052 - val_acc: 0.9991\n",
      "Epoch 64/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0067 - acc: 0.9986 - val_loss: 0.0540 - val_acc: 0.9944\n",
      "Epoch 65/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0137 - val_acc: 0.9981\n",
      "Epoch 66/100\n",
      "20249/20249 [==============================] - 70s - loss: 2.1744e-04 - acc: 1.0000 - val_loss: 0.0115 - val_acc: 0.9991\n",
      "Epoch 67/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0111 - val_acc: 0.9981\n",
      "Epoch 68/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0276 - val_acc: 0.9972\n",
      "Epoch 69/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0041 - acc: 0.9991 - val_loss: 0.0177 - val_acc: 0.9972\n",
      "Epoch 70/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0058 - acc: 0.9987 - val_loss: 0.0409 - val_acc: 0.9925\n",
      "Epoch 71/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0683 - val_acc: 0.9887\n",
      "Epoch 72/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0132 - val_acc: 0.9981\n",
      "Epoch 73/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0499 - val_acc: 0.9934\n",
      "Epoch 74/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0159 - acc: 0.9968 - val_loss: 0.0325 - val_acc: 0.9944\n",
      "Epoch 75/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0037 - acc: 0.9992 - val_loss: 0.0147 - val_acc: 0.9972\n",
      "Epoch 76/100\n",
      "20249/20249 [==============================] - 71s - loss: 3.9406e-04 - acc: 0.9998 - val_loss: 0.0078 - val_acc: 0.9972\n",
      "Epoch 77/100\n",
      "20249/20249 [==============================] - 71s - loss: 7.6152e-04 - acc: 0.9999 - val_loss: 0.0070 - val_acc: 0.9981\n",
      "Epoch 78/100\n",
      "20249/20249 [==============================] - 70s - loss: 6.4457e-04 - acc: 0.9997 - val_loss: 0.0082 - val_acc: 0.9981\n",
      "Epoch 79/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0010 - acc: 0.9996 - val_loss: 0.0177 - val_acc: 0.9981\n",
      "Epoch 80/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0150 - val_acc: 0.9962\n",
      "Epoch 81/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0049 - acc: 0.9988 - val_loss: 0.0213 - val_acc: 0.9972\n",
      "Epoch 82/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0117 - acc: 0.9975 - val_loss: 0.0708 - val_acc: 0.9934\n",
      "Epoch 83/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0074 - acc: 0.9987 - val_loss: 0.0271 - val_acc: 0.9981\n",
      "Epoch 84/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0027 - acc: 0.9996 - val_loss: 0.0214 - val_acc: 0.9972\n",
      "Epoch 85/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0024 - acc: 0.9997 - val_loss: 0.0061 - val_acc: 0.9981\n",
      "Epoch 86/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0137 - val_acc: 0.9972\n",
      "Epoch 87/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0053 - acc: 0.9992 - val_loss: 0.0154 - val_acc: 0.9981\n",
      "Epoch 88/100\n",
      "20249/20249 [==============================] - 70s - loss: 7.6128e-04 - acc: 0.9998 - val_loss: 0.0202 - val_acc: 0.9981\n",
      "Epoch 89/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0037 - acc: 0.9992 - val_loss: 0.0178 - val_acc: 0.9981\n",
      "Epoch 90/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0018 - acc: 0.9997 - val_loss: 0.0114 - val_acc: 0.9962\n",
      "Epoch 91/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0040 - val_acc: 0.9981\n",
      "Epoch 92/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0069 - acc: 0.9989 - val_loss: 0.0360 - val_acc: 0.9953\n",
      "Epoch 93/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0135 - acc: 0.9977 - val_loss: 0.0024 - val_acc: 0.9991\n",
      "Epoch 94/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0073 - val_acc: 0.9972\n",
      "Epoch 95/100\n",
      "20249/20249 [==============================] - 70s - loss: 2.3982e-04 - acc: 0.9999 - val_loss: 0.0155 - val_acc: 0.9991\n",
      "Epoch 96/100\n",
      "20249/20249 [==============================] - 71s - loss: 2.2243e-04 - acc: 1.0000 - val_loss: 0.0228 - val_acc: 0.9972\n",
      "Epoch 97/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0013 - acc: 0.9999 - val_loss: 0.0233 - val_acc: 0.9962\n",
      "Epoch 98/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0161 - val_acc: 0.9981\n",
      "Epoch 99/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0263 - val_acc: 0.9953\n",
      "Epoch 100/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0171 - acc: 0.9970 - val_loss: 0.0293 - val_acc: 0.9972\n",
      "Model has accuracy: 99.5539696699 %\n",
      "Name: 165_163_163_200\tKernelsize: (5, 3, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 71, 96, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 35, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 33, 46, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 23, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 21, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 7, 10, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1120)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 200)               224200    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 229,457\n",
      "Trainable params: 229,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Created path: ../Data/2017-09-17/165_163_163_200/\n",
      "Train on 20249 samples, validate on 1066 samples\n",
      "Epoch 1/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.5706 - acc: 0.7062 - val_loss: 0.4541 - val_acc: 0.8124\n",
      "Epoch 2/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.4344 - acc: 0.7999 - val_loss: 0.3417 - val_acc: 0.8480\n",
      "Epoch 3/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.3703 - acc: 0.8334 - val_loss: 0.2911 - val_acc: 0.8687\n",
      "Epoch 4/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.3081 - acc: 0.8682 - val_loss: 0.2426 - val_acc: 0.9081\n",
      "Epoch 5/100\n",
      "20249/20249 [==============================] - 72s - loss: 0.2271 - acc: 0.9043 - val_loss: 0.1555 - val_acc: 0.9418\n",
      "Epoch 6/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.1483 - acc: 0.9410 - val_loss: 0.0817 - val_acc: 0.9737\n",
      "Epoch 7/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0956 - acc: 0.9636 - val_loss: 0.0647 - val_acc: 0.9794\n",
      "Epoch 8/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0677 - acc: 0.9766 - val_loss: 0.0452 - val_acc: 0.9869\n",
      "Epoch 9/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0504 - acc: 0.9828 - val_loss: 0.0488 - val_acc: 0.9878\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 71s - loss: 0.0515 - acc: 0.9805 - val_loss: 0.0401 - val_acc: 0.9897\n",
      "Epoch 11/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0265 - acc: 0.9911 - val_loss: 0.0376 - val_acc: 0.9925\n",
      "Epoch 12/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0361 - val_acc: 0.9944\n",
      "Epoch 13/100\n",
      "20249/20249 [==============================] - 71s - loss: 0.0167 - acc: 0.9941 - val_loss: 0.0234 - val_acc: 0.9953\n",
      "Epoch 14/100\n",
      "20249/20249 [==============================] - 70s - loss: 0.0256 - acc: 0.9916 - val_loss: 0.0252 - val_acc: 0.9953\n",
      "Epoch 15/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0100 - acc: 0.9968 - val_loss: 0.0189 - val_acc: 0.9972\n",
      "Epoch 16/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0160 - acc: 0.9945 - val_loss: 0.0360 - val_acc: 0.9878\n",
      "Epoch 17/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0095 - acc: 0.9968 - val_loss: 0.0368 - val_acc: 0.9916\n",
      "Epoch 18/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0298 - val_acc: 0.9962\n",
      "Epoch 19/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0379 - val_acc: 0.9906\n",
      "Epoch 20/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0306 - val_acc: 0.9925\n",
      "Epoch 21/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0127 - acc: 0.9957 - val_loss: 0.0338 - val_acc: 0.9916\n",
      "Epoch 22/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0268 - val_acc: 0.9953\n",
      "Epoch 23/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0226 - val_acc: 0.9962\n",
      "Epoch 24/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0238 - val_acc: 0.9962\n",
      "Epoch 25/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0091 - acc: 0.9971 - val_loss: 0.0166 - val_acc: 0.9962\n",
      "Epoch 26/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0163 - acc: 0.9947 - val_loss: 0.0235 - val_acc: 0.9944\n",
      "Epoch 27/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0038 - acc: 0.9985 - val_loss: 0.0192 - val_acc: 0.9925\n",
      "Epoch 28/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0117 - acc: 0.9958 - val_loss: 0.0418 - val_acc: 0.9934\n",
      "Epoch 29/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0211 - val_acc: 0.9944\n",
      "Epoch 30/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0151 - val_acc: 0.9962\n",
      "Epoch 31/100\n",
      "20249/20249 [==============================] - 69s - loss: 3.6870e-04 - acc: 1.0000 - val_loss: 0.0191 - val_acc: 0.9953\n",
      "Epoch 32/100\n",
      "20249/20249 [==============================] - 69s - loss: 3.1118e-04 - acc: 0.9999 - val_loss: 0.0120 - val_acc: 0.9991\n",
      "Epoch 33/100\n",
      "20249/20249 [==============================] - 69s - loss: 3.0365e-04 - acc: 1.0000 - val_loss: 0.0189 - val_acc: 0.9972\n",
      "Epoch 34/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0256 - acc: 0.9927 - val_loss: 0.0163 - val_acc: 0.9981\n",
      "Epoch 35/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0256 - val_acc: 0.9944\n",
      "Epoch 36/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0200 - val_acc: 0.9972\n",
      "Epoch 37/100\n",
      "20249/20249 [==============================] - 68s - loss: 8.2953e-04 - acc: 0.9997 - val_loss: 0.0201 - val_acc: 0.9962\n",
      "Epoch 38/100\n",
      "20249/20249 [==============================] - 68s - loss: 2.7664e-04 - acc: 1.0000 - val_loss: 0.0189 - val_acc: 0.9962\n",
      "Epoch 39/100\n",
      "20249/20249 [==============================] - 68s - loss: 1.0569e-04 - acc: 1.0000 - val_loss: 0.0209 - val_acc: 0.9962\n",
      "Epoch 40/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0276 - val_acc: 0.9944\n",
      "Epoch 41/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0760 - val_acc: 0.9869\n",
      "Epoch 42/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0132 - acc: 0.9963 - val_loss: 0.0171 - val_acc: 0.9972\n",
      "Epoch 43/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0156 - val_acc: 0.9991\n",
      "Epoch 44/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0154 - val_acc: 0.9991\n",
      "Epoch 45/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0171 - val_acc: 0.9981\n",
      "Epoch 46/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0217 - val_acc: 0.9953\n",
      "Epoch 47/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0121 - acc: 0.9964 - val_loss: 0.0154 - val_acc: 0.9991\n",
      "Epoch 48/100\n",
      "20249/20249 [==============================] - 68s - loss: 3.1362e-04 - acc: 1.0000 - val_loss: 0.0159 - val_acc: 0.9991\n",
      "Epoch 49/100\n",
      "20249/20249 [==============================] - 68s - loss: 8.8143e-04 - acc: 0.9998 - val_loss: 0.0195 - val_acc: 0.9962\n",
      "Epoch 50/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0058 - acc: 0.9985 - val_loss: 0.0277 - val_acc: 0.9962\n",
      "Epoch 51/100\n",
      "20249/20249 [==============================] - 68s - loss: 9.7189e-04 - acc: 0.9998 - val_loss: 0.0219 - val_acc: 0.9953\n",
      "Epoch 52/100\n",
      "20249/20249 [==============================] - 68s - loss: 6.2993e-04 - acc: 0.9999 - val_loss: 0.0174 - val_acc: 0.9972\n",
      "Epoch 53/100\n",
      "20249/20249 [==============================] - 68s - loss: 7.6133e-05 - acc: 1.0000 - val_loss: 0.0208 - val_acc: 0.9953\n",
      "Epoch 54/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0285 - val_acc: 0.9962\n",
      "Epoch 55/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0171 - val_acc: 0.9981\n",
      "Epoch 56/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0016 - acc: 0.9993 - val_loss: 0.0193 - val_acc: 0.9972\n",
      "Epoch 57/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.7626e-04 - acc: 1.0000 - val_loss: 0.0232 - val_acc: 0.9962\n",
      "Epoch 58/100\n",
      "20249/20249 [==============================] - 69s - loss: 2.2051e-04 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9991\n",
      "Epoch 59/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0106 - acc: 0.9964 - val_loss: 0.0339 - val_acc: 0.9944\n",
      "Epoch 60/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0248 - val_acc: 0.9962\n",
      "Epoch 61/100\n",
      "20249/20249 [==============================] - 68s - loss: 7.0765e-04 - acc: 0.9998 - val_loss: 0.0195 - val_acc: 0.9962\n",
      "Epoch 62/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0201 - val_acc: 0.9962\n",
      "Epoch 63/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0087 - acc: 0.9979 - val_loss: 0.0116 - val_acc: 0.9962\n",
      "Epoch 64/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0126 - val_acc: 0.9972\n",
      "Epoch 65/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0020 - acc: 0.9992 - val_loss: 0.0352 - val_acc: 0.9953\n",
      "Epoch 66/100\n",
      "20249/20249 [==============================] - 69s - loss: 4.9814e-04 - acc: 0.9999 - val_loss: 0.0182 - val_acc: 0.9972\n",
      "Epoch 67/100\n",
      "20249/20249 [==============================] - 69s - loss: 3.9244e-04 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9981\n",
      "Epoch 68/100\n",
      "20249/20249 [==============================] - 69s - loss: 7.6266e-05 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 0.9991\n",
      "Epoch 69/100\n",
      "20249/20249 [==============================] - 69s - loss: 7.8431e-05 - acc: 1.0000 - val_loss: 0.0173 - val_acc: 0.9981\n",
      "Epoch 70/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.4238e-04 - acc: 1.0000 - val_loss: 0.0153 - val_acc: 0.9991\n",
      "Epoch 71/100\n",
      "20249/20249 [==============================] - 68s - loss: 5.2075e-05 - acc: 1.0000 - val_loss: 0.0222 - val_acc: 0.9972\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 69s - loss: 4.6813e-04 - acc: 0.9999 - val_loss: 0.0290 - val_acc: 0.9962\n",
      "Epoch 73/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0127 - acc: 0.9969 - val_loss: 0.0204 - val_acc: 0.9972\n",
      "Epoch 74/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0182 - val_acc: 0.9981\n",
      "Epoch 75/100\n",
      "20249/20249 [==============================] - 68s - loss: 6.9117e-04 - acc: 0.9998 - val_loss: 0.0235 - val_acc: 0.9953\n",
      "Epoch 76/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0308 - val_acc: 0.9972\n",
      "Epoch 77/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0198 - val_acc: 0.9981\n",
      "Epoch 78/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0171 - val_acc: 0.9981\n",
      "Epoch 79/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0072 - acc: 0.9981 - val_loss: 0.0156 - val_acc: 0.9991\n",
      "Epoch 80/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0157 - val_acc: 0.9991\n",
      "Epoch 81/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0153 - val_acc: 0.9991\n",
      "Epoch 82/100\n",
      "20249/20249 [==============================] - 68s - loss: 2.2287e-04 - acc: 0.9999 - val_loss: 0.0172 - val_acc: 0.9981\n",
      "Epoch 83/100\n",
      "20249/20249 [==============================] - 69s - loss: 4.5726e-05 - acc: 1.0000 - val_loss: 0.0156 - val_acc: 0.9991\n",
      "Epoch 84/100\n",
      "20249/20249 [==============================] - 69s - loss: 4.2433e-04 - acc: 0.9999 - val_loss: 0.0154 - val_acc: 0.9991\n",
      "Epoch 85/100\n",
      "20249/20249 [==============================] - 68s - loss: 6.2123e-05 - acc: 1.0000 - val_loss: 0.0151 - val_acc: 0.9991\n",
      "Epoch 86/100\n",
      "20249/20249 [==============================] - 69s - loss: 7.6017e-04 - acc: 0.9997 - val_loss: 0.0213 - val_acc: 0.9972\n",
      "Epoch 87/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0131 - acc: 0.9968 - val_loss: 0.0184 - val_acc: 0.9981\n",
      "Epoch 88/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0150 - val_acc: 0.9962\n",
      "Epoch 89/100\n",
      "20249/20249 [==============================] - 69s - loss: 7.8928e-04 - acc: 0.9998 - val_loss: 0.0173 - val_acc: 0.9981\n",
      "Epoch 90/100\n",
      "20249/20249 [==============================] - 69s - loss: 1.4540e-04 - acc: 1.0000 - val_loss: 0.0179 - val_acc: 0.9972\n",
      "Epoch 91/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0155 - val_acc: 0.9991\n",
      "Epoch 92/100\n",
      "20249/20249 [==============================] - 68s - loss: 6.9280e-05 - acc: 1.0000 - val_loss: 0.0168 - val_acc: 0.9981\n",
      "Epoch 93/100\n",
      "20249/20249 [==============================] - 69s - loss: 2.0846e-05 - acc: 1.0000 - val_loss: 0.0171 - val_acc: 0.9981\n",
      "Epoch 94/100\n",
      "20249/20249 [==============================] - 69s - loss: 0.0064 - acc: 0.9989 - val_loss: 0.0291 - val_acc: 0.9953\n",
      "Epoch 95/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0164 - val_acc: 0.9991\n",
      "Epoch 96/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0233 - val_acc: 0.9962\n",
      "Epoch 97/100\n",
      "20249/20249 [==============================] - 68s - loss: 6.7339e-04 - acc: 0.9998 - val_loss: 0.0305 - val_acc: 0.9981\n",
      "Epoch 98/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0100 - acc: 0.9983 - val_loss: 0.0167 - val_acc: 0.9981\n",
      "Epoch 99/100\n",
      "20249/20249 [==============================] - 68s - loss: 3.5975e-04 - acc: 0.9999 - val_loss: 0.0229 - val_acc: 0.9972\n",
      "Epoch 100/100\n",
      "20249/20249 [==============================] - 68s - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0232 - val_acc: 0.9981\n",
      "Model has accuracy: 99.910793934 %\n"
     ]
    }
   ],
   "source": [
    "for name, k_size in zip(names_3Conv_1Fully, kernel_sizes_3Conv_1Fully):\n",
    "    print('Name: {}\\tKernelsize: {}'.format(name, k_size))\n",
    "    #print('k_sizes: {},{},{}'.format(k_size[0], k_size[1], k_size[2]))\n",
    "    \n",
    "    ## Path for Model saving!\n",
    "    path_model = '../TrainedModels/' + str(datetime.now().strftime('%Y-%m-%d')) + '/'\n",
    "\n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "        print('Created path: {}'.format(path_model))\n",
    "    \n",
    "    #### MODEL ####\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional layer initialised with random input weights\n",
    "    model.add(Conv2D(16, (k_size[0], k_size[0]), kernel_initializer=RandomNormal(mean=0, stddev=1/(h*w)), padding='valid', input_shape=input_shape, activation='selu'))\n",
    "    # Reduce size a bit\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    mult_shape1 = np.prod(model.layers[1].output_shape[1:])\n",
    "\n",
    "    # Second Convolutional layer\n",
    "    model.add(Conv2D(16, (k_size[1], k_size[1]), kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape1), padding='valid', activation='selu'))\n",
    "    # Reduce size a bit\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    mult_shape2 = np.prod(model.layers[3].output_shape[1:])\n",
    "\n",
    "    # Third Convolutional layer\n",
    "    model.add(Conv2D(16, (k_size[2], k_size[2]), kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape2), padding='valid', activation='selu'))\n",
    "    # Reduce size a bit\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    #Converting the 2D images to 1D vectors\n",
    "    model.add(Flatten())  \n",
    "    mult_shape3 = np.prod(model.layers[6].output_shape[1:])\n",
    "\n",
    "    # First Fully connected layer\n",
    "    model.add(Dense(200, activation='selu', kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    mult_shape4 = np.prod(model.layers[8].output_shape[1:])\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape4)))\n",
    "    \n",
    "    #### END OF MODEL ####\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    ## Save the Model and create Data path!\n",
    "    path_data = '../Data/{}/{}/'.format(str(datetime.now().strftime('%Y-%m-%d')), name )\n",
    "\n",
    "    if not os.path.exists(path_data):\n",
    "        os.makedirs(path_data)\n",
    "        print('Created path: {}'.format(path_data))\n",
    "\n",
    "    # Open the file\n",
    "    with open(path_data + 'ModelSummary.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "        \n",
    "    \n",
    "    #Defining optimiser and compiling the model\n",
    "    model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "                  optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = LossHistory()\n",
    "    \n",
    "    ## Start the training!\n",
    "    model.fit(X_train_norm, Y_train,  batch_size=76, epochs=100, verbose=1, validation_split = 0.05, callbacks=[history])\n",
    "\n",
    "    #Evaluating trained model on test images\n",
    "    score = model.evaluate(X_test_norm, Y_test, verbose=0)\n",
    "    print('Model has accuracy:', score[1]*100,'%')\n",
    "    \n",
    "    # Save trained NN\n",
    "    model.save(path_model + name + '.h5')\n",
    "    np.savetxt(path_model + name + '_TrainingData.txt', np.transpose([history.accuracy, history.losses]))\n",
    "    \n",
    "    ### Performance on Testdata! ###\n",
    "    # Get probs per image\n",
    "    probs = []\n",
    "\n",
    "    i=0 \n",
    "    for img in X_test_norm:\n",
    "        #plt.imshow(np.squeeze(img), cmap='gray')\n",
    "\n",
    "        img = np.reshape(img, [1,h,w,1])\n",
    "        p = model.predict_proba(img, verbose=0)\n",
    "        probs.append(np.squeeze(p))\n",
    "\n",
    "        #name = \"Good img\" if Y_test[i, 1] == 1 else \"Bad img\"\n",
    "        #plt.title('{}. Score: [{:.4f}, {:.4f}]'.format(name, p[0,0], p[0,1]))\n",
    "        #plt.savefig('../Plots/TestImg/{}.png'.format(i))\n",
    "        i+=1\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    \n",
    "    np.savetxt(path_data + 'ScoresOnTestData.txt', np.transpose([Y_test, probs]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with 2 Conv and 2 Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_2Conv_2Fully = ['167_163_200_200', '165_163_200_200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_sizes_2Conv_2Fully = [(7,3), (5,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 167_163_200_200\tKernelsize: (7, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 69, 94, 16)        800       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 34, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 45, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 16, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 5632)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 200)               1126600   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,170,121\n",
      "Trainable params: 1,170,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Created path: ../Data/2017-09-17/167_163_200_200/\n",
      "Train on 20249 samples, validate on 1066 samples\n",
      "Epoch 1/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.5259 - acc: 0.7340 - val_loss: 0.3785 - val_acc: 0.8452\n",
      "Epoch 2/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.3521 - acc: 0.8367 - val_loss: 0.2683 - val_acc: 0.8940\n",
      "Epoch 3/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.2381 - acc: 0.9012 - val_loss: 0.1624 - val_acc: 0.9400\n",
      "Epoch 4/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.1730 - acc: 0.9328 - val_loss: 0.1354 - val_acc: 0.9475\n",
      "Epoch 5/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.1282 - acc: 0.9510 - val_loss: 0.0793 - val_acc: 0.9681\n",
      "Epoch 6/100\n",
      "20249/20249 [==============================] - 65s - loss: 0.0898 - acc: 0.9681 - val_loss: 0.0643 - val_acc: 0.9765\n",
      "Epoch 7/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0736 - acc: 0.9731 - val_loss: 0.0542 - val_acc: 0.9831\n",
      "Epoch 8/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0538 - acc: 0.9813 - val_loss: 0.0559 - val_acc: 0.9803\n",
      "Epoch 9/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0581 - acc: 0.9801 - val_loss: 0.0498 - val_acc: 0.9822\n",
      "Epoch 10/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0323 - acc: 0.9892 - val_loss: 0.0267 - val_acc: 0.9897\n",
      "Epoch 11/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0415 - acc: 0.9863 - val_loss: 0.0455 - val_acc: 0.9841\n",
      "Epoch 12/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0250 - acc: 0.9912 - val_loss: 0.0474 - val_acc: 0.9850\n",
      "Epoch 13/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0265 - acc: 0.9905 - val_loss: 0.0283 - val_acc: 0.9887\n",
      "Epoch 14/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0250 - acc: 0.9905 - val_loss: 0.0206 - val_acc: 0.9906\n",
      "Epoch 15/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0215 - acc: 0.9933 - val_loss: 0.0097 - val_acc: 0.9934\n",
      "Epoch 16/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0209 - acc: 0.9923 - val_loss: 0.0073 - val_acc: 0.9972\n",
      "Epoch 17/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0153 - acc: 0.9943 - val_loss: 0.0128 - val_acc: 0.9953\n",
      "Epoch 18/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0117 - acc: 0.9960 - val_loss: 0.0208 - val_acc: 0.9887\n",
      "Epoch 19/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0114 - val_acc: 0.9934\n",
      "Epoch 20/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0169 - acc: 0.9941 - val_loss: 0.0155 - val_acc: 0.9972\n",
      "Epoch 21/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0180 - val_acc: 0.9934\n",
      "Epoch 22/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0109 - val_acc: 0.9962\n",
      "Epoch 23/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0076 - acc: 0.9968 - val_loss: 0.0087 - val_acc: 0.9944\n",
      "Epoch 24/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0098 - acc: 0.9965 - val_loss: 0.0202 - val_acc: 0.9953\n",
      "Epoch 25/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0136 - acc: 0.9957 - val_loss: 0.0594 - val_acc: 0.9850\n",
      "Epoch 26/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0158 - acc: 0.9943 - val_loss: 0.0106 - val_acc: 0.9934\n",
      "Epoch 27/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0102 - acc: 0.9968 - val_loss: 0.0250 - val_acc: 0.9962\n",
      "Epoch 28/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0056 - acc: 0.9982 - val_loss: 0.0142 - val_acc: 0.9944\n",
      "Epoch 29/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0087 - acc: 0.9975 - val_loss: 0.0048 - val_acc: 0.9981\n",
      "Epoch 30/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0124 - acc: 0.9961 - val_loss: 0.0226 - val_acc: 0.9925\n",
      "Epoch 31/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0221 - val_acc: 0.9962\n",
      "Epoch 32/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0169 - val_acc: 0.9981\n",
      "Epoch 33/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0177 - val_acc: 0.9944\n",
      "Epoch 34/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0133 - val_acc: 0.9953\n",
      "Epoch 35/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0174 - acc: 0.9951 - val_loss: 0.0043 - val_acc: 0.9981\n",
      "Epoch 36/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0101 - acc: 0.9968 - val_loss: 0.0054 - val_acc: 0.9981\n",
      "Epoch 37/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0065 - val_acc: 0.9962\n",
      "Epoch 38/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0049 - val_acc: 0.9972\n",
      "Epoch 39/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0280 - val_acc: 0.9944\n",
      "Epoch 40/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0094 - acc: 0.9972 - val_loss: 0.0150 - val_acc: 0.9972\n",
      "Epoch 41/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0183 - val_acc: 0.9944\n",
      "Epoch 42/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0227 - val_acc: 0.9925\n",
      "Epoch 43/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0108 - acc: 0.9972 - val_loss: 0.0206 - val_acc: 0.9916\n",
      "Epoch 44/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0197 - val_acc: 0.9981\n",
      "Epoch 45/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0152 - val_acc: 0.9962\n",
      "Epoch 46/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0162 - val_acc: 0.9981\n",
      "Epoch 47/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0054 - val_acc: 0.9962\n",
      "Epoch 48/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0062 - val_acc: 0.9981\n",
      "Epoch 49/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0073 - val_acc: 0.9962\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 64s - loss: 0.0162 - acc: 0.9956 - val_loss: 0.0120 - val_acc: 0.9981\n",
      "Epoch 51/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0091 - acc: 0.9971 - val_loss: 0.0290 - val_acc: 0.9934\n",
      "Epoch 52/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0077 - val_acc: 0.9981\n",
      "Epoch 53/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0127 - val_acc: 0.9972\n",
      "Epoch 54/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0117 - val_acc: 0.9981\n",
      "Epoch 55/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0089 - acc: 0.9972 - val_loss: 0.0095 - val_acc: 0.9981\n",
      "Epoch 56/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0101 - val_acc: 0.9972\n",
      "Epoch 57/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0097 - val_acc: 0.9962\n",
      "Epoch 58/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0040 - val_acc: 0.9991\n",
      "Epoch 59/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0276 - val_acc: 0.9934\n",
      "Epoch 60/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0149 - acc: 0.9969 - val_loss: 0.0124 - val_acc: 0.9962\n",
      "Epoch 61/100\n",
      "20249/20249 [==============================] - 65s - loss: 0.0055 - acc: 0.9986 - val_loss: 0.0161 - val_acc: 0.9972\n",
      "Epoch 62/100\n",
      "20249/20249 [==============================] - 65s - loss: 0.0138 - acc: 0.9967 - val_loss: 0.0117 - val_acc: 0.9991\n",
      "Epoch 63/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0173 - acc: 0.9962 - val_loss: 0.0292 - val_acc: 0.9953\n",
      "Epoch 64/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0079 - acc: 0.9978 - val_loss: 0.0088 - val_acc: 0.9962\n",
      "Epoch 65/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0192 - val_acc: 0.9981\n",
      "Epoch 66/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0211 - val_acc: 0.9981\n",
      "Epoch 67/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0163 - val_acc: 0.9981\n",
      "Epoch 68/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0164 - val_acc: 0.9972\n",
      "Epoch 69/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0165 - val_acc: 0.9981\n",
      "Epoch 70/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0123 - val_acc: 0.9981\n",
      "Epoch 71/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0127 - acc: 0.9968 - val_loss: 0.0119 - val_acc: 0.9962\n",
      "Epoch 72/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0066 - val_acc: 0.9972\n",
      "Epoch 73/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0067 - val_acc: 0.9972\n",
      "Epoch 74/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0048 - val_acc: 0.9981\n",
      "Epoch 75/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0057 - acc: 0.9985 - val_loss: 0.0169 - val_acc: 0.9962\n",
      "Epoch 76/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0256 - val_acc: 0.9972\n",
      "Epoch 77/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0074 - acc: 0.9985 - val_loss: 0.0165 - val_acc: 0.9972\n",
      "Epoch 78/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0073 - acc: 0.9982 - val_loss: 4.0401e-04 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0076 - val_acc: 0.9981\n",
      "Epoch 80/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0170 - acc: 0.9964 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0179 - val_acc: 0.9981\n",
      "Epoch 82/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0219 - acc: 0.9962 - val_loss: 0.0963 - val_acc: 0.9916\n",
      "Epoch 83/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0138 - acc: 0.9973 - val_loss: 0.0206 - val_acc: 0.9962\n",
      "Epoch 84/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0318 - val_acc: 0.9962\n",
      "Epoch 85/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0360 - val_acc: 0.9962\n",
      "Epoch 86/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0196 - val_acc: 0.9962\n",
      "Epoch 87/100\n",
      "20249/20249 [==============================] - 64s - loss: 6.7667e-04 - acc: 0.9997 - val_loss: 0.0278 - val_acc: 0.9962\n",
      "Epoch 88/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0209 - val_acc: 0.9953\n",
      "Epoch 89/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0017 - val_acc: 0.9991\n",
      "Epoch 90/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0071 - val_acc: 0.9981\n",
      "Epoch 91/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0168 - acc: 0.9976 - val_loss: 0.0041 - val_acc: 0.9981\n",
      "Epoch 92/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0193 - acc: 0.9961 - val_loss: 0.0272 - val_acc: 0.9953\n",
      "Epoch 93/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0086 - acc: 0.9979 - val_loss: 0.0267 - val_acc: 0.9962\n",
      "Epoch 94/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0054 - acc: 0.9983 - val_loss: 0.0402 - val_acc: 0.9953\n",
      "Epoch 95/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0398 - val_acc: 0.9962\n",
      "Epoch 96/100\n",
      "20249/20249 [==============================] - 63s - loss: 0.0046 - acc: 0.9988 - val_loss: 0.0374 - val_acc: 0.9972\n",
      "Epoch 97/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0243 - val_acc: 0.9962\n",
      "Epoch 98/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0050 - acc: 0.9990 - val_loss: 0.0655 - val_acc: 0.9953\n",
      "Epoch 99/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0149 - acc: 0.9972 - val_loss: 0.0199 - val_acc: 0.9972\n",
      "Epoch 100/100\n",
      "20249/20249 [==============================] - 64s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0245 - val_acc: 0.9972\n",
      "Model has accuracy: 99.732381802 %\n",
      "Name: 165_163_200_200\tKernelsize: (5, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 71, 96, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 35, 48, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 33, 46, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 16, 23, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 5888)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 200)               1177800   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 1,220,937\n",
      "Trainable params: 1,220,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Created path: ../Data/2017-09-17/165_163_200_200/\n",
      "Train on 20249 samples, validate on 1066 samples\n",
      "Epoch 1/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.5395 - acc: 0.7282 - val_loss: 0.3961 - val_acc: 0.8077\n",
      "Epoch 2/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.3804 - acc: 0.8201 - val_loss: 0.2958 - val_acc: 0.8621\n",
      "Epoch 3/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.2873 - acc: 0.8709 - val_loss: 0.2089 - val_acc: 0.9146\n",
      "Epoch 4/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.2280 - acc: 0.8991 - val_loss: 0.1805 - val_acc: 0.9315\n",
      "Epoch 5/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.1751 - acc: 0.9290 - val_loss: 0.1131 - val_acc: 0.9512\n",
      "Epoch 6/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.1417 - acc: 0.9444 - val_loss: 0.0986 - val_acc: 0.9653\n",
      "Epoch 7/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.1169 - acc: 0.9539 - val_loss: 0.1017 - val_acc: 0.9634\n",
      "Epoch 8/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.1015 - acc: 0.9608 - val_loss: 0.0786 - val_acc: 0.9747\n",
      "Epoch 9/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0795 - acc: 0.9693 - val_loss: 0.0941 - val_acc: 0.9644\n",
      "Epoch 10/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0698 - acc: 0.9742 - val_loss: 0.0626 - val_acc: 0.9784\n",
      "Epoch 11/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0681 - acc: 0.9757 - val_loss: 0.0455 - val_acc: 0.9841\n",
      "Epoch 12/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0554 - acc: 0.9783 - val_loss: 0.0424 - val_acc: 0.9841\n",
      "Epoch 13/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0483 - acc: 0.9826 - val_loss: 0.0475 - val_acc: 0.9822\n",
      "Epoch 14/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0462 - acc: 0.9826 - val_loss: 0.0300 - val_acc: 0.9887\n",
      "Epoch 15/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0406 - acc: 0.9856 - val_loss: 0.0434 - val_acc: 0.9897\n",
      "Epoch 16/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0363 - acc: 0.9873 - val_loss: 0.0386 - val_acc: 0.9897\n",
      "Epoch 17/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0340 - acc: 0.9874 - val_loss: 0.0302 - val_acc: 0.9869\n",
      "Epoch 18/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0261 - acc: 0.9912 - val_loss: 0.0296 - val_acc: 0.9916\n",
      "Epoch 19/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0293 - acc: 0.9899 - val_loss: 0.0286 - val_acc: 0.9887\n",
      "Epoch 20/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0243 - acc: 0.9911 - val_loss: 0.0220 - val_acc: 0.9925\n",
      "Epoch 21/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0349 - acc: 0.9883 - val_loss: 0.0214 - val_acc: 0.9916\n",
      "Epoch 22/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0211 - acc: 0.9927 - val_loss: 0.0191 - val_acc: 0.9925\n",
      "Epoch 23/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0227 - acc: 0.9919 - val_loss: 0.0259 - val_acc: 0.9925\n",
      "Epoch 24/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0222 - acc: 0.9920 - val_loss: 0.0268 - val_acc: 0.9916\n",
      "Epoch 25/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0196 - acc: 0.9929 - val_loss: 0.0192 - val_acc: 0.9953\n",
      "Epoch 26/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0202 - acc: 0.9926 - val_loss: 0.0222 - val_acc: 0.9897\n",
      "Epoch 27/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0180 - val_acc: 0.9934\n",
      "Epoch 28/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0161 - acc: 0.9951 - val_loss: 0.0195 - val_acc: 0.9934\n",
      "Epoch 29/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0219 - acc: 0.9927 - val_loss: 0.0155 - val_acc: 0.9934\n",
      "Epoch 30/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0112 - acc: 0.9963 - val_loss: 0.0078 - val_acc: 0.9972\n",
      "Epoch 31/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0122 - val_acc: 0.9944\n",
      "Epoch 32/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0189 - val_acc: 0.9934\n",
      "Epoch 33/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0111 - acc: 0.9960 - val_loss: 0.0130 - val_acc: 0.9953\n",
      "Epoch 34/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0101 - val_acc: 0.9953\n",
      "Epoch 35/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0162 - acc: 0.9949 - val_loss: 0.0120 - val_acc: 0.9962\n",
      "Epoch 36/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0125 - acc: 0.9960 - val_loss: 0.0161 - val_acc: 0.9953\n",
      "Epoch 37/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0087 - acc: 0.9971 - val_loss: 0.0065 - val_acc: 0.9991\n",
      "Epoch 38/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0184 - acc: 0.9947 - val_loss: 0.0232 - val_acc: 0.9934\n",
      "Epoch 39/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0120 - acc: 0.9956 - val_loss: 0.0158 - val_acc: 0.9953\n",
      "Epoch 40/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0112 - val_acc: 0.9972\n",
      "Epoch 41/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0127 - acc: 0.9960 - val_loss: 0.0330 - val_acc: 0.9906\n",
      "Epoch 42/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0243 - acc: 0.9931 - val_loss: 0.0080 - val_acc: 0.9944\n",
      "Epoch 43/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0066 - val_acc: 0.9981\n",
      "Epoch 44/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0073 - acc: 0.9979 - val_loss: 0.0048 - val_acc: 0.9991\n",
      "Epoch 45/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0043 - acc: 0.9983 - val_loss: 0.0069 - val_acc: 0.9981\n",
      "Epoch 46/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0027 - val_acc: 0.9991\n",
      "Epoch 47/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0175 - val_acc: 0.9962\n",
      "Epoch 48/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0165 - acc: 0.9951 - val_loss: 0.0070 - val_acc: 0.9962\n",
      "Epoch 49/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0110 - acc: 0.9968 - val_loss: 0.0047 - val_acc: 0.9981\n",
      "Epoch 50/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0101 - acc: 0.9964 - val_loss: 0.0100 - val_acc: 0.9962\n",
      "Epoch 51/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0412 - val_acc: 0.9953\n",
      "Epoch 52/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0110 - val_acc: 0.9972\n",
      "Epoch 53/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0080 - acc: 0.9971 - val_loss: 0.0119 - val_acc: 0.9991\n",
      "Epoch 54/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0148 - acc: 0.9963 - val_loss: 0.0172 - val_acc: 0.9944\n",
      "Epoch 55/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0066 - acc: 0.9985 - val_loss: 0.0173 - val_acc: 0.9981\n",
      "Epoch 56/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0194 - acc: 0.9951 - val_loss: 0.0222 - val_acc: 0.9953\n",
      "Epoch 57/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0093 - acc: 0.9974 - val_loss: 0.0106 - val_acc: 0.9981\n",
      "Epoch 58/100\n",
      "20249/20249 [==============================] - 66s - loss: 8.0814e-04 - acc: 0.9999 - val_loss: 0.0038 - val_acc: 0.9991\n",
      "Epoch 59/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0076 - acc: 0.9984 - val_loss: 0.0145 - val_acc: 0.9953\n",
      "Epoch 60/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0201 - acc: 0.9946 - val_loss: 0.0152 - val_acc: 0.9944\n",
      "Epoch 61/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0105 - acc: 0.9972 - val_loss: 0.0193 - val_acc: 0.9972\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249/20249 [==============================] - 67s - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0129 - val_acc: 0.9972\n",
      "Epoch 63/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0101 - val_acc: 0.9972\n",
      "Epoch 64/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0146 - val_acc: 0.9972\n",
      "Epoch 65/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0076 - val_acc: 0.9972\n",
      "Epoch 66/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0074 - acc: 0.9981 - val_loss: 0.0110 - val_acc: 0.9953\n",
      "Epoch 68/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0119 - val_acc: 0.9981\n",
      "Epoch 69/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0133 - val_acc: 0.9962\n",
      "Epoch 70/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0034 - acc: 0.9993 - val_loss: 0.0086 - val_acc: 0.9972\n",
      "Epoch 71/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0031 - val_acc: 0.9981\n",
      "Epoch 72/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0136 - acc: 0.9958 - val_loss: 0.0186 - val_acc: 0.9962\n",
      "Epoch 73/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0076 - acc: 0.9980 - val_loss: 0.0173 - val_acc: 0.9953\n",
      "Epoch 74/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0138 - acc: 0.9963 - val_loss: 0.0245 - val_acc: 0.9925\n",
      "Epoch 75/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0058 - acc: 0.9984 - val_loss: 0.0025 - val_acc: 0.9991\n",
      "Epoch 76/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0157 - val_acc: 0.9962\n",
      "Epoch 77/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0063 - acc: 0.9985 - val_loss: 0.0276 - val_acc: 0.9944\n",
      "Epoch 78/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0133 - acc: 0.9961 - val_loss: 0.0155 - val_acc: 0.9981\n",
      "Epoch 79/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0097 - acc: 0.9972 - val_loss: 0.0081 - val_acc: 0.9991\n",
      "Epoch 80/100\n",
      "20249/20249 [==============================] - 67s - loss: 7.7813e-04 - acc: 0.9998 - val_loss: 0.0053 - val_acc: 0.9991\n",
      "Epoch 81/100\n",
      "20249/20249 [==============================] - 66s - loss: 4.7208e-04 - acc: 0.9999 - val_loss: 0.0074 - val_acc: 0.9991\n",
      "Epoch 82/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0036 - acc: 0.9995 - val_loss: 0.0113 - val_acc: 0.9972\n",
      "Epoch 83/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0234 - acc: 0.9943 - val_loss: 0.0187 - val_acc: 0.9972\n",
      "Epoch 84/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0122 - acc: 0.9973 - val_loss: 0.0081 - val_acc: 0.9962\n",
      "Epoch 85/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0044 - acc: 0.9991 - val_loss: 0.0094 - val_acc: 0.9972\n",
      "Epoch 86/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0037 - val_acc: 0.9991\n",
      "Epoch 87/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0081 - val_acc: 0.9972\n",
      "Epoch 88/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0055 - val_acc: 0.9991\n",
      "Epoch 89/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0113 - acc: 0.9977 - val_loss: 0.0111 - val_acc: 0.9972\n",
      "Epoch 90/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0028 - val_acc: 0.9991\n",
      "Epoch 91/100\n",
      "20249/20249 [==============================] - 66s - loss: 2.3788e-04 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9981\n",
      "Epoch 92/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0095 - acc: 0.9974 - val_loss: 0.0021 - val_acc: 0.9991\n",
      "Epoch 93/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0059 - val_acc: 0.9962\n",
      "Epoch 94/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0386 - acc: 0.9910 - val_loss: 0.0223 - val_acc: 0.9962\n",
      "Epoch 95/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0036 - acc: 0.9986 - val_loss: 0.0174 - val_acc: 0.9981\n",
      "Epoch 96/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0098 - val_acc: 0.9991\n",
      "Epoch 97/100\n",
      "20249/20249 [==============================] - 67s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0070 - val_acc: 0.9981\n",
      "Epoch 98/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0092 - val_acc: 0.9991\n",
      "Epoch 99/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0174 - val_acc: 0.9944\n",
      "Epoch 100/100\n",
      "20249/20249 [==============================] - 66s - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0076 - val_acc: 0.9962\n",
      "Model has accuracy: 99.3755575379 %\n"
     ]
    }
   ],
   "source": [
    "for name, k_size in zip(names_2Conv_2Fully, kernel_sizes_2Conv_2Fully):\n",
    "    print('Name: {}\\tKernelsize: {}'.format(name, k_size))\n",
    "    #print('k_sizes: {},{}'.format(k_size[0], k_size[1]))\n",
    "    \n",
    "    ## Path for Model saving!\n",
    "    path_model = '../TrainedModels/' + str(datetime.now().strftime('%Y-%m-%d')) + '/'\n",
    "\n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "        print('Created path: {}'.format(path_model))\n",
    "    \n",
    "    #### MODEL ####\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional layer initialised with random input weights\n",
    "    model.add(Conv2D(16, (k_size[0], k_size[0]), kernel_initializer=RandomNormal(mean=0, stddev=1/(h*w)), padding='valid', input_shape=input_shape, activation='selu'))\n",
    "    # Reduce size a bit\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    mult_shape1 = np.prod(model.layers[1].output_shape[1:])\n",
    "\n",
    "    # Second Convolutional layer\n",
    "    model.add(Conv2D(16, (k_size[1], k_size[1]), kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape1), padding='valid', activation='selu'))\n",
    "    # Reduce size a bit\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    #Converting the 2D images to 1D vectors\n",
    "    model.add(Flatten())  \n",
    "    mult_shape2 = np.prod(model.layers[4].output_shape[1:])\n",
    "\n",
    "    # First Fully connected layer\n",
    "    model.add(Dense(200, activation='selu', kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    mult_shape3 = np.prod(model.layers[6].output_shape[1:])\n",
    "\n",
    "    model.add(Dense(200, activation='selu', kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    mult_shape4 = np.prod(model.layers[8].output_shape[1:])\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer=RandomNormal(mean=0, stddev=1/mult_shape4)))\n",
    "    \n",
    "    #### END OF MODEL ####\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    ## Save the Model and create Data path!\n",
    "    path_data = '../Data/{}/{}/'.format(str(datetime.now().strftime('%Y-%m-%d')), name )\n",
    "\n",
    "    if not os.path.exists(path_data):\n",
    "        os.makedirs(path_data)\n",
    "        print('Created path: {}'.format(path_data))\n",
    "\n",
    "    # Open the file\n",
    "    with open(path_data + 'ModelSummary.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "        \n",
    "    \n",
    "    #Defining optimiser and compiling the model\n",
    "    model.compile(loss='binary_crossentropy', # using the cross-entropy loss function\n",
    "                  optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = LossHistory()\n",
    "    \n",
    "    ## Start the training!\n",
    "    model.fit(X_train_norm, Y_train,  batch_size=76, epochs=100, verbose=1, validation_split = 0.05, callbacks=[history])\n",
    "\n",
    "    #Evaluating trained model on test images\n",
    "    score = model.evaluate(X_test_norm, Y_test, verbose=0)\n",
    "    print('Model has accuracy:', score[1]*100,'%')\n",
    "    \n",
    "    # Save trained NN\n",
    "    model.save(path_model + name + '.h5')\n",
    "    np.savetxt(path_model + name + '_TrainingData.txt', np.transpose([history.accuracy, history.losses]))\n",
    "    \n",
    "    ### Performance on Testdata! ###\n",
    "    # Get probs per image\n",
    "    probs = []\n",
    "\n",
    "    i=0 \n",
    "    for img in X_test_norm:\n",
    "        #plt.imshow(np.squeeze(img), cmap='gray')\n",
    "\n",
    "        img = np.reshape(img, [1,h,w,1])\n",
    "        p = model.predict_proba(img, verbose=0)\n",
    "        probs.append(np.squeeze(p))\n",
    "\n",
    "        #name = \"Good img\" if Y_test[i, 1] == 1 else \"Bad img\"\n",
    "        #plt.title('{}. Score: [{:.4f}, {:.4f}]'.format(name, p[0,0], p[0,1]))\n",
    "        #plt.savefig('../Plots/TestImg/{}.png'.format(i))\n",
    "        i+=1\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    \n",
    "    np.savetxt(path_data + 'ScoresOnTestData.txt', np.transpose([Y_test, probs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
